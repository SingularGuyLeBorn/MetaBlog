---
title: Understanding Transformers
date: 2023-10-27
tags:
  - Deep Learning
  - NLP
---

# Understanding Transformers

The Transformer architecture has revolutionized Natural Language Processing.

## Key Concepts

- **Self-Attention**: The mechanism that allows the model to weigh the importance of different words.
- **Positional Encoding**: Since Transformers process words in parallel, they need a way to know the order of words.

See also: [[Attention Mechanism]]

<KnowledgeGraph />
