{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¤– MetaUniverse LLM API æµ‹è¯•\n",
    "\n",
    "æµ‹è¯•å¤šå‚å•† LLM Provider æ˜¯å¦å¯ç”¨ï¼šOpenAI, Anthropic, Gemini, æ™ºè°±, DeepSeek, Qwen, Kimi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. åŠ è½½ç¯å¢ƒå˜é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ .env æ–‡ä»¶\n",
    "load_dotenv()\n",
    "\n",
    "# æŸ¥çœ‹é…ç½®çš„ providers\n",
    "providers = []\n",
    "if os.getenv('OPENAI_API_KEY'):\n",
    "    providers.append('openai')\n",
    "if os.getenv('ANTHROPIC_API_KEY'):\n",
    "    providers.append('anthropic')\n",
    "if os.getenv('GEMINI_API_KEY'):\n",
    "    providers.append('gemini')\n",
    "if os.getenv('ZHIPU_API_KEY'):\n",
    "    providers.append('zhipu')\n",
    "if os.getenv('DEEPSEEK_API_KEY'):\n",
    "    providers.append('deepseek')\n",
    "if os.getenv('QWEN_API_KEY'):\n",
    "    providers.append('qwen')\n",
    "if os.getenv('KIMI_API_KEY'):\n",
    "    providers.append('kimi')\n",
    "\n",
    "print(f\"âœ… å·²é…ç½®çš„ Providers: {providers}\")\n",
    "print(f\"ğŸ“Š é»˜è®¤ Provider: {os.getenv('LLM_DEFAULT_PROVIDER', 'æœªè®¾ç½®')}\")\n",
    "print(f\"ğŸ’° æ¯æ—¥é¢„ç®—: ${os.getenv('LLM_DAILY_BUDGET', '10')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æµ‹è¯• OpenAI (GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def test_openai():\n",
    "    api_key = os.getenv('OPENAI_API_KEY')\n",
    "    if not api_key:\n",
    "        print(\"âš ï¸ æœªé…ç½® OPENAI_API_KEY\")\n",
    "        return\n",
    "    \n",
    "    base_url = os.getenv('OPENAI_BASE_URL', 'https://api.openai.com/v1')\n",
    "    model = os.getenv('OPENAI_MODEL', 'gpt-4o')\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{base_url}/chat/completions\",\n",
    "            headers={\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"},\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": \"Hello, this is a test. Reply with 'OpenAI OK'\"}],\n",
    "                \"max_tokens\": 20\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            content = data['choices'][0]['message']['content']\n",
    "            usage = data['usage']\n",
    "            cost = (usage['prompt_tokens'] + usage['completion_tokens']) / 1000 * 0.005\n",
    "            print(f\"âœ… OpenAI OK\")\n",
    "            print(f\"   Model: {data['model']}\")\n",
    "            print(f\"   Response: {content[:50]}...\")\n",
    "            print(f\"   Tokens: {usage['total_tokens']}\")\n",
    "            print(f\"   Cost: ${cost:.4f}\")\n",
    "        else:\n",
    "            print(f\"âŒ OpenAI Error: {response.status_code} - {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ OpenAI Exception: {e}\")\n",
    "\n",
    "test_openai()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æµ‹è¯• Anthropic (Claude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_anthropic():\n",
    "    api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "    if not api_key:\n",
    "        print(\"âš ï¸ æœªé…ç½® ANTHROPIC_API_KEY\")\n",
    "        return\n",
    "    \n",
    "    base_url = os.getenv('ANTHROPIC_BASE_URL', 'https://api.anthropic.com/v1')\n",
    "    model = os.getenv('ANTHROPIC_MODEL', 'claude-3-5-sonnet-20241022')\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{base_url}/messages\",\n",
    "            headers={\n",
    "                \"x-api-key\": api_key,\n",
    "                \"anthropic-version\": \"2023-06-01\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            },\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"max_tokens\": 20,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": \"Reply with 'Anthropic OK'\"}]\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            content = data['content'][0]['text']\n",
    "            usage = data['usage']\n",
    "            cost = (usage['input_tokens'] + usage['output_tokens']) / 1000 * 0.003\n",
    "            print(f\"âœ… Anthropic OK\")\n",
    "            print(f\"   Model: {data['model']}\")\n",
    "            print(f\"   Response: {content[:50]}...\")\n",
    "            print(f\"   Tokens: {usage['input_tokens'] + usage['output_tokens']}\")\n",
    "            print(f\"   Cost: ${cost:.4f}\")\n",
    "        else:\n",
    "            print(f\"âŒ Anthropic Error: {response.status_code} - {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Anthropic Exception: {e}\")\n",
    "\n",
    "test_anthropic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æµ‹è¯• Google Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gemini():\n",
    "    api_key = os.getenv('GEMINI_API_KEY')\n",
    "    if not api_key:\n",
    "        print(\"âš ï¸ æœªé…ç½® GEMINI_API_KEY\")\n",
    "        return\n",
    "    \n",
    "    base_url = os.getenv('GEMINI_BASE_URL', 'https://generativelanguage.googleapis.com/v1beta')\n",
    "    model = os.getenv('GEMINI_MODEL', 'gemini-1.5-pro')\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{base_url}/models/{model}:generateContent?key={api_key}\",\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            json={\n",
    "                \"contents\": [{\"parts\": [{\"text\": \"Reply with 'Gemini OK'\"}]}],\n",
    "                \"generationConfig\": {\"maxOutputTokens\": 20}\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            content = data['candidates'][0]['content']['parts'][0]['text']\n",
    "            usage = data.get('usageMetadata', {})\n",
    "            total_tokens = usage.get('promptTokenCount', 0) + usage.get('candidatesTokenCount', 0)\n",
    "            cost = total_tokens / 1000 * 0.0035\n",
    "            print(f\"âœ… Gemini OK\")\n",
    "            print(f\"   Model: {model}\")\n",
    "            print(f\"   Response: {content[:50]}...\")\n",
    "            print(f\"   Tokens: {total_tokens}\")\n",
    "            print(f\"   Cost: ${cost:.4f}\")\n",
    "        else:\n",
    "            print(f\"âŒ Gemini Error: {response.status_code} - {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Gemini Exception: {e}\")\n",
    "\n",
    "test_gemini()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æµ‹è¯•æ™ºè°±æ¸…è¨€ (Zhipu/GLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_zhipu():\n",
    "    api_key = os.getenv('ZHIPU_API_KEY')\n",
    "    if not api_key:\n",
    "        print(\"âš ï¸ æœªé…ç½® ZHIPU_API_KEY\")\n",
    "        return\n",
    "    \n",
    "    base_url = os.getenv('ZHIPU_BASE_URL', 'https://open.bigmodel.cn/api/paas/v4')\n",
    "    model = os.getenv('ZHIPU_MODEL', 'glm-4')\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{base_url}/chat/completions\",\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {api_key}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            },\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": \"å›å¤'æ™ºè°±OK'\"}],\n",
    "                \"max_tokens\": 20\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            content = data['choices'][0]['message']['content']\n",
    "            usage = data['usage']\n",
    "            cost = usage['total_tokens'] / 1000 * 0.0014\n",
    "            print(f\"âœ… æ™ºè°± OK\")\n",
    "            print(f\"   Model: {data['model']}\")\n",
    "            print(f\"   Response: {content[:50]}...\")\n",
    "            print(f\"   Tokens: {usage['total_tokens']}\")\n",
    "            print(f\"   Cost: ${cost:.4f}\")\n",
    "        else:\n",
    "            print(f\"âŒ æ™ºè°± Error: {response.status_code} - {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ™ºè°± Exception: {e}\")\n",
    "\n",
    "test_zhipu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æµ‹è¯• DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_deepseek():\n",
    "    api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "    if not api_key:\n",
    "        print(\"âš ï¸ æœªé…ç½® DEEPSEEK_API_KEY\")\n",
    "        return\n",
    "    \n",
    "    base_url = os.getenv('DEEPSEEK_BASE_URL', 'https://api.deepseek.com/v1')\n",
    "    model = os.getenv('DEEPSEEK_MODEL', 'deepseek-chat')\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{base_url}/chat/completions\",\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {api_key}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            },\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": \"å›å¤'DeepSeek OK'\"}],\n",
    "                \"max_tokens\": 20\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            content = data['choices'][0]['message']['content']\n",
    "            usage = data['usage']\n",
    "            cost = usage['total_tokens'] / 1000 * 0.00027\n",
    "            print(f\"âœ… DeepSeek OK\")\n",
    "            print(f\"   Model: {data['model']}\")\n",
    "            print(f\"   Response: {content[:50]}...\")\n",
    "            print(f\"   Tokens: {usage['total_tokens']}\")\n",
    "            print(f\"   Cost: ${cost:.4f}\")\n",
    "        else:\n",
    "            print(f\"âŒ DeepSeek Error: {response.status_code} - {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ DeepSeek Exception: {e}\")\n",
    "\n",
    "test_deepseek()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. æµ‹è¯•é˜¿é‡Œäº‘ Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_qwen():\n",
    "    api_key = os.getenv('QWEN_API_KEY')\n",
    "    if not api_key:\n",
    "        print(\"âš ï¸ æœªé…ç½® QWEN_API_KEY\")\n",
    "        return\n",
    "    \n",
    "    base_url = os.getenv('QWEN_BASE_URL', 'https://dashscope.aliyuncs.com/api/v1')\n",
    "    model = os.getenv('QWEN_MODEL', 'qwen-plus')\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{base_url}/services/aigc/text-generation/generation\",\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {api_key}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            },\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"input\": {\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": \"å›å¤'Qwen OK'\"}]\n",
    "                },\n",
    "                \"parameters\": {\n",
    "                    \"max_tokens\": 20,\n",
    "                    \"result_format\": \"message\"\n",
    "                }\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            content = data['output']['choices'][0]['message']['content']\n",
    "            usage = data['usage']\n",
    "            total_tokens = usage['input_tokens'] + usage['output_tokens']\n",
    "            cost = total_tokens / 1000 * 0.0008\n",
    "            print(f\"âœ… Qwen OK\")\n",
    "            print(f\"   Model: {model}\")\n",
    "            print(f\"   Response: {content[:50]}...\")\n",
    "            print(f\"   Tokens: {total_tokens}\")\n",
    "            print(f\"   Cost: ${cost:.4f}\")\n",
    "        else:\n",
    "            print(f\"âŒ Qwen Error: {response.status_code} - {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Qwen Exception: {e}\")\n",
    "\n",
    "test_qwen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. æµ‹è¯• Kimi (Moonshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_kimi():\n",
    "    api_key = os.getenv('KIMI_API_KEY')\n",
    "    if not api_key:\n",
    "        print(\"âš ï¸ æœªé…ç½® KIMI_API_KEY\")\n",
    "        return\n",
    "    \n",
    "    base_url = os.getenv('KIMI_BASE_URL', 'https://api.moonshot.cn/v1')\n",
    "    model = os.getenv('KIMI_MODEL', 'kimi-latest')\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{base_url}/chat/completions\",\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {api_key}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            },\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": \"å›å¤'Kimi OK'\"}],\n",
    "                \"max_tokens\": 20\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            content = data['choices'][0]['message']['content']\n",
    "            usage = data['usage']\n",
    "            cost = usage['total_tokens'] / 1000 * 0.003\n",
    "            print(f\"âœ… Kimi OK\")\n",
    "            print(f\"   Model: {data['model']}\")\n",
    "            print(f\"   Response: {content[:50]}...\")\n",
    "            print(f\"   Tokens: {usage['total_tokens']}\")\n",
    "            print(f\"   Cost: ${cost:.4f}\")\n",
    "        else:\n",
    "            print(f\"âŒ Kimi Error: {response.status_code} - {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Kimi Exception: {e}\")\n",
    "\n",
    "test_kimi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. æ‰¹é‡æµ‹è¯•æ‰€æœ‰ API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸš€ æ‰¹é‡æµ‹è¯•æ‰€æœ‰å·²é…ç½®çš„ LLM API\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "test_openai()\n",
    "print()\n",
    "test_anthropic()\n",
    "print()\n",
    "test_gemini()\n",
    "print()\n",
    "test_zhipu()\n",
    "print()\n",
    "test_deepseek()\n",
    "print()\n",
    "test_qwen()\n",
    "print()\n",
    "test_kimi()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"âœ¨ æµ‹è¯•å®Œæˆï¼\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. æµ‹è¯•æ–‡ç« ç”ŸæˆåŠŸèƒ½\n",
    "\n",
    "ä½¿ç”¨å®é™…çš„ Agent æŠ€èƒ½ç”Ÿæˆä¸€ç¯‡æµ‹è¯•æ–‡ç« "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def test_write_article():\n",
    "    \"\"\"æµ‹è¯• WriteArticle æŠ€èƒ½\"\"\"\n",
    "    default_provider = os.getenv('LLM_DEFAULT_PROVIDER', 'openai')\n",
    "    \n",
    "    print(f\"ğŸ“ æµ‹è¯•æ–‡ç« ç”Ÿæˆï¼ˆä½¿ç”¨ {default_provider}ï¼‰...\")\n",
    "    \n",
    "    # æ„å»ºæµ‹è¯•è¯·æ±‚\n",
    "    topic = \"ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ ï¼ˆç®€è¦ä»‹ç»ï¼‰\"\n",
    "    \n",
    "    # ä½¿ç”¨é»˜è®¤ provider ç›´æ¥è°ƒç”¨\n",
    "    llm = get_llm_manager()\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯åšå®¢ä½œè€…ã€‚ä¸ºç»™å®šçš„ä¸»é¢˜ç”Ÿæˆç®€çŸ­çš„æµ‹è¯•æ–‡ç« ï¼ˆ300å­—ä»¥å†…ï¼‰ã€‚\"},\n",
    "        {\"role\": \"user\", \"content\": f\"è¯·å†™ä¸€ç¯‡å…³äº '{topic}' çš„ç®€çŸ­ä»‹ç»æ–‡ç« ï¼š\"}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        response = llm.chat({\"messages\": messages})\n",
    "        print(f\"âœ… æ–‡ç« ç”ŸæˆæˆåŠŸï¼\")\n",
    "        print(f\"   Tokens: {response['usage']['total_tokens']}\")\n",
    "        print(f\"   Cost: ${response['cost']:.4f}\")\n",
    "        print(f\"\\nğŸ“„ æ–‡ç« å†…å®¹é¢„è§ˆï¼š\")\n",
    "        print(f\"{response['content'][:300]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ–‡ç« ç”Ÿæˆå¤±è´¥: {e}\")\n",
    "\n",
    "def get_llm_manager():\n",
    "    \"\"\"è·å– LLM Manager å®ä¾‹\"\"\"\n",
    "    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥å¯¼å…¥ TypeScript æ¨¡å—\n",
    "    # åœ¨å®é™…ä½¿ç”¨æ—¶ï¼Œå¯ä»¥é€šè¿‡ Node.js è°ƒç”¨æˆ–æµè§ˆå™¨æ§åˆ¶å°\n",
    "    class MockLLMManager:\n",
    "        def chat(self, request):\n",
    "            # è°ƒç”¨å®é™…çš„ API\n",
    "            provider = os.getenv('LLM_DEFAULT_PROVIDER', 'openai')\n",
    "            \n",
    "            # æ ¹æ® provider è°ƒç”¨å¯¹åº”æµ‹è¯•å‡½æ•°\n",
    "            if provider == 'openai':\n",
    "                api_key = os.getenv('OPENAI_API_KEY')\n",
    "                base_url = os.getenv('OPENAI_BASE_URL', 'https://api.openai.com/v1')\n",
    "                model = os.getenv('OPENAI_MODEL', 'gpt-4o')\n",
    "                \n",
    "                response = requests.post(\n",
    "                    f\"{base_url}/chat/completions\",\n",
    "                    headers={\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"},\n",
    "                    json={\n",
    "                        \"model\": model,\n",
    "                        \"messages\": request[\"messages\"],\n",
    "                        \"max_tokens\": 500\n",
    "                    },\n",
    "                    timeout=60\n",
    "                )\n",
    "                \n",
    "                data = response.json()\n",
    "                usage = data['usage']\n",
    "                cost = (usage['prompt_tokens'] + usage['completion_tokens']) / 1000 * 0.005\n",
    "                \n",
    "                return {\n",
    "                    'content': data['choices'][0]['message']['content'],\n",
    "                    'usage': usage,\n",
    "                    'cost': cost\n",
    "                }\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Provider {provider} not implemented in test\")\n",
    "    \n",
    "    return MockLLMManager()\n",
    "\n",
    "# è¿è¡Œæµ‹è¯•\n",
    "if os.getenv('OPENAI_API_KEY') or os.getenv('LLM_DEFAULT_PROVIDER'):\n",
    "    test_write_article()\n",
    "else:\n",
    "    print(\"âš ï¸ æœªé…ç½® API Keyï¼Œè·³è¿‡æ–‡ç« ç”Ÿæˆæµ‹è¯•\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“‹ æ€»ç»“\n",
    "\n",
    "è¿è¡Œä»¥ä¸Šæµ‹è¯•åï¼Œä½ å¯ä»¥ï¼š\n",
    "\n",
    "1. **ç¡®è®¤ API å¯ç”¨æ€§** - æŸ¥çœ‹å“ªäº› provider å¯ä»¥æ­£å¸¸è°ƒç”¨\n",
    "2. **ä¼°ç®—æˆæœ¬** - äº†è§£ä¸åŒæ¨¡å‹çš„ Token ä»·æ ¼å’Œå®é™…èŠ±è´¹\n",
    "3. **æµ‹è¯•å“åº”è´¨é‡** - æ£€æŸ¥å„æ¨¡å‹çš„å›å¤è´¨é‡\n",
    "4. **éªŒè¯é…ç½®** - ç¡®ä¿ .env æ–‡ä»¶é…ç½®æ­£ç¡®\n",
    "\n",
    "å¦‚æœæŸäº›æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ï¼š\n",
    "- API Key æ˜¯å¦æ­£ç¡®\n",
    "- è´¦æˆ·æ˜¯å¦æœ‰è¶³å¤Ÿä½™é¢\n",
    "- ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\n",
    "- Base URL æ˜¯å¦éœ€è¦ä»£ç†"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
