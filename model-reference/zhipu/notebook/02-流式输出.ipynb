{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ™ºè°± AI æµå¼è¾“å‡ºæµ‹è¯•\n",
    "\n",
    "æœ¬ Notebook æ¼”ç¤ºæ™ºè°± AI æµå¼è¾“å‡º (Streaming) åŠŸèƒ½çš„ä½¿ç”¨æ–¹æ³•å’Œæ•ˆæœã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒé…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from typing import Iterator, Optional, Tuple\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "\n",
    "# API é…ç½®\n",
    "API_KEY = \"f7ec61a7856a4f4389e037eb588c69df.0pZgcFlTCk9GHCXM\"\n",
    "BASE_URL = \"https://open.bigmodel.cn/api/paas/v4/chat/completions\"\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒé…ç½®å®Œæˆ\")\n",
    "print(f\"API ç«¯ç‚¹: {BASE_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 æµå¼å“åº”è§£æå°è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_chat_completion(\n",
    "    model: str, \n",
    "    messages: list, \n",
    "    api_key: str = API_KEY,\n",
    "    show_realtime: bool = True\n",
    ") -> Tuple[str, Optional[str], float]:\n",
    "    \"\"\"\n",
    "    æµå¼å¯¹è¯å®Œæˆ\n",
    "    \n",
    "    Args:\n",
    "        model: æ¨¡å‹ä»£ç \n",
    "        messages: æ¶ˆæ¯åˆ—è¡¨\n",
    "        api_key: API å¯†é’¥\n",
    "        show_realtime: æ˜¯å¦å®æ—¶æ˜¾ç¤ºè¾“å‡º\n",
    "    \n",
    "    Returns:\n",
    "        (å®Œæ•´å†…å®¹, æ€ç»´é“¾å†…å®¹, æ€»è€—æ—¶ç§’)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = requests.post(\n",
    "        url=BASE_URL,\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        },\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": True\n",
    "        },\n",
    "        stream=True,\n",
    "        timeout=(10, 300)\n",
    "    )\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    \n",
    "    full_content = \"\"\n",
    "    full_reasoning = \"\"\n",
    "    first_token_time = None\n",
    "    \n",
    "    if show_realtime:\n",
    "        print(f\"\\nğŸ¤– [{model}] å®æ—¶è¾“å‡º:\\n\" + \"-\"*50)\n",
    "        print(\"ğŸ’¬ \", end=\"\", flush=True)\n",
    "    \n",
    "    for line in response.iter_lines():\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        line = line.decode('utf-8')\n",
    "        \n",
    "        if not line.startswith('data: '):\n",
    "            continue\n",
    "        \n",
    "        data = line[6:]  # å»æ‰ \"data: \" å‰ç¼€\n",
    "        \n",
    "        # æ£€æŸ¥ç»“æŸæ ‡è®°\n",
    "        if data == '[DONE]':\n",
    "            if show_realtime:\n",
    "                print(f\"\\n\" + \"-\"*50)\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            chunk = json.loads(data)\n",
    "            choice = chunk.get('choices', [{}])[0]\n",
    "            delta = choice.get('delta', {})\n",
    "            \n",
    "            # è®°å½•é¦– token æ—¶é—´\n",
    "            if first_token_time is None:\n",
    "                first_token_time = time.time() - start_time\n",
    "            \n",
    "            # æå–å†…å®¹\n",
    "            content = delta.get('content', '')\n",
    "            if content:\n",
    "                full_content += content\n",
    "                if show_realtime:\n",
    "                    print(content, end=\"\", flush=True)\n",
    "            \n",
    "            # æå–æ€ç»´é“¾ï¼ˆä»… Thinking æ¨¡å‹ï¼‰\n",
    "            reasoning = delta.get('reasoning_content', '')\n",
    "            if reasoning:\n",
    "                full_reasoning += reasoning\n",
    "                \n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    if show_realtime:\n",
    "        print(f\"\\n\\nâ±ï¸ é¦– token å»¶è¿Ÿ: {first_token_time:.3f}s\")\n",
    "        print(f\"â±ï¸ æ€»è€—æ—¶: {total_time:.3f}s\")\n",
    "        print(f\"ğŸ“ æ€»å­—ç¬¦æ•°: {len(full_content)}\")\n",
    "    \n",
    "    return full_content, full_reasoning if full_reasoning else None, total_time\n",
    "\n",
    "\n",
    "def non_stream_chat_completion(\n",
    "    model: str, \n",
    "    messages: list, \n",
    "    api_key: str = API_KEY\n",
    ") -> Tuple[str, Optional[str], float]:\n",
    "    \"\"\"éæµå¼å¯¹è¯å®Œæˆï¼ˆç”¨äºå¯¹æ¯”ï¼‰\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = requests.post(\n",
    "        url=BASE_URL,\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        },\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": False\n",
    "        },\n",
    "        timeout=(10, 300)\n",
    "    )\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    choice = result.get('choices', [{}])[0]\n",
    "    message = choice.get('message', {})\n",
    "    content = message.get('content', '')\n",
    "    reasoning = message.get('reasoning_content', '')\n",
    "    \n",
    "    return content, reasoning if reasoning else None, total_time\n",
    "\n",
    "print(\"âœ… å°è£…å‡½æ•°å®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. GLM-4.7-Flash æµå¼è¾“å‡ºæµ‹è¯•\n",
    "\n",
    "**æ¨¡å‹ä¿¡æ¯ï¼š**\n",
    "- æ¨¡å‹ä»£ç ï¼š`glm-4.7-flash`\n",
    "- ä¸Šä¸‹æ–‡ï¼š200K\n",
    "- æœ€å¤§è¾“å‡ºï¼š128K\n",
    "- ç‰¹ç‚¹ï¼šè¶…é•¿ä¸Šä¸‹æ–‡ï¼Œé€‚åˆé•¿æ–‡æ¡£å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•ï¼šç®€å•é—®å€™\n",
    "messages = [{\"role\": \"user\", \"content\": \"ä½ å¥½ï¼Œè¯·ç®€çŸ­ä»‹ç»ä¸€ä¸‹è‡ªå·±\"}]\n",
    "content, reasoning, duration = stream_chat_completion(\"glm-4.7-flash\", messages)\n",
    "\n",
    "print(f\"\\nğŸ“¦ æ€ç»´é“¾: {reasoning if reasoning else 'æ— '}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•ï¼šé•¿æ–‡æœ¬ç”Ÿæˆ\n",
    "messages = [{\"role\": \"user\", \"content\": \"å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½çš„çŸ­æ–‡ï¼ˆçº¦200å­—ï¼‰\"}]\n",
    "content, reasoning, duration = stream_chat_completion(\"glm-4.7-flash\", messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. GLM-4-Flash-250414 æµå¼è¾“å‡ºæµ‹è¯•\n",
    "\n",
    "**æ¨¡å‹ä¿¡æ¯ï¼š**\n",
    "- æ¨¡å‹ä»£ç ï¼š`glm-4-flash-250414`\n",
    "- ä¸Šä¸‹æ–‡ï¼š128K\n",
    "- æœ€å¤§è¾“å‡ºï¼š16K\n",
    "- ç‰¹ç‚¹ï¼šè½»é‡çº§ï¼Œå“åº”é€Ÿåº¦å¿«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•ï¼šä»£ç ç”Ÿæˆ\n",
    "messages = [{\n",
    "    \"role\": \"user\", \n",
    "    \"content\": \"ç”¨ Python å†™ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•°ï¼Œå¹¶åŠ ä¸Šæ³¨é‡Š\"\n",
    "}]\n",
    "content, reasoning, duration = stream_chat_completion(\"glm-4-flash-250414\", messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•ï¼šå¤šè½®å¯¹è¯\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯...\"},\n",
    "    {\"role\": \"user\", \"content\": \"èƒ½ä¸¾ä¸ªä¾‹å­å—ï¼Ÿ\"}\n",
    "]\n",
    "content, reasoning, duration = stream_chat_completion(\"glm-4-flash-250414\", messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. GLM-4.6V-Flash æµå¼è¾“å‡ºæµ‹è¯•ï¼ˆè§†è§‰æ¨¡å‹ï¼‰\n",
    "\n",
    "**æ¨¡å‹ä¿¡æ¯ï¼š**\n",
    "- æ¨¡å‹ä»£ç ï¼š`glm-4.6v-flash`\n",
    "- ä¸Šä¸‹æ–‡ï¼š128K\n",
    "- æœ€å¤§è¾“å‡ºï¼š32K\n",
    "- ç‰¹ç‚¹ï¼šæ”¯æŒå›¾ç‰‡è¾“å…¥ï¼Œè‡ªåŠ¨åˆ¤æ–­æ˜¯å¦æ€è€ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_vision_chat(\n",
    "    model: str,\n",
    "    image_url: str,\n",
    "    prompt: str,\n",
    "    api_key: str = API_KEY\n",
    ") -> Tuple[str, float]:\n",
    "    \"\"\"è§†è§‰æ¨¡å‹æµå¼å¯¹è¯\"\"\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n",
    "            {\"type\": \"text\", \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "    \n",
    "    content, _, duration = stream_chat_completion(model, messages, api_key)\n",
    "    return content, duration\n",
    "\n",
    "print(\"âœ… è§†è§‰æ¨¡å‹å°è£…å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•ï¼šä½¿ç”¨ç¤ºä¾‹å›¾ç‰‡\n",
    "# ä½¿ç”¨æ™ºè°±å®˜æ–¹ç¤ºä¾‹å›¾ç‰‡\n",
    "sample_image = \"https://img.alicdn.com/imgextra/i1/O1CN01xRdqwB1M3UOxVL2sO_!!6000000001380-2-tps-1200-1200.png\"\n",
    "\n",
    "content, duration = stream_vision_chat(\n",
    "    \"glm-4.6v-flash\",\n",
    "    sample_image,\n",
    "    \"è¯·æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹\"\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“· è§†è§‰ç†è§£å®Œæˆï¼Œè€—æ—¶: {duration:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•ï¼šåŸºäºå›¾ç‰‡çš„æ¨ç†\n",
    "content, duration = stream_vision_chat(\n",
    "    \"glm-4.6v-flash\",\n",
    "    sample_image,\n",
    "    \"è¿™å¼ å›¾ç‰‡å¯èƒ½ç”¨äºä»€ä¹ˆåœºæ™¯ï¼Ÿ\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. GLM-4.1V-Thinking-Flash æµå¼è¾“å‡ºæµ‹è¯•ï¼ˆæ€è€ƒæ¨¡å‹ï¼‰\n",
    "\n",
    "**æ¨¡å‹ä¿¡æ¯ï¼š**\n",
    "- æ¨¡å‹ä»£ç ï¼š`glm-4.1v-thinking-flash`\n",
    "- ä¸Šä¸‹æ–‡ï¼š64K\n",
    "- æœ€å¤§è¾“å‡ºï¼š16K\n",
    "- ç‰¹ç‚¹ï¼š**æ”¯æŒæ€ç»´é“¾ (reasoning_content)**ï¼Œå¯å±•ç¤ºæ¨ç†è¿‡ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•ï¼šæ•°å­¦é—®é¢˜ï¼ˆå¸¦æ€ç»´é“¾å±•ç¤ºï¼‰\n",
    "messages = [{\"role\": \"user\", \"content\": \"è§£æ–¹ç¨‹ï¼š3x + 7 = 22ï¼Œè¯·å±•ç¤ºæ€è€ƒè¿‡ç¨‹\"}]\n",
    "\n",
    "print(\"ğŸ”¢ æ•°å­¦é—®é¢˜æ±‚è§£\\n\" + \"=\"*60)\n",
    "content, reasoning, duration = stream_chat_completion(\"glm-4.1v-thinking-flash\", messages)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"\\nğŸ§  æ€ç»´é“¾ï¼ˆæ¨ç†è¿‡ç¨‹ï¼‰:\\n{reasoning}\")\n",
    "print(f\"\\nğŸ’¡ æœ€ç»ˆç­”æ¡ˆ:\\n{content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•ï¼šé€»è¾‘æ¨ç†\n",
    "messages = [{\n",
    "    \"role\": \"user\", \n",
    "    \"content\": \"\"\"\n",
    "    é€»è¾‘æ¨ç†é¢˜ï¼š\n",
    "    æœ‰ä¸‰ä¸ªäºº Aã€Bã€Cï¼Œåˆ†åˆ«æ¥è‡ªåŒ—äº¬ã€ä¸Šæµ·ã€å¹¿å·ã€‚\n",
    "    å·²çŸ¥ï¼š\n",
    "    1. A ä¸æ˜¯åŒ—äº¬äºº\n",
    "    2. B ä¸æ˜¯ä¸Šæµ·äºº\n",
    "    3. åŒ—äº¬äººä¸æ˜¯æœ€å¹´è½»çš„\n",
    "    4. B å¹´é¾„æœ€å¤§\n",
    "    è¯·é—®æ¯ä¸ªäººæ¥è‡ªå“ªä¸ªåŸå¸‚ï¼Ÿ\n",
    "    \"\"\"\n",
    "}]\n",
    "\n",
    "content, reasoning, duration = stream_chat_completion(\"glm-4.1v-thinking-flash\", messages)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"\\nğŸ§  æ€ç»´é“¾:\\n{reasoning}\")\n",
    "print(f\"\\nğŸ’¡ ç­”æ¡ˆ:\\n{content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. æµå¼å“åº”è§£æå°è£…ï¼ˆé«˜çº§ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingParser:\n",
    "    \"\"\"æµå¼å“åº”è§£æå™¨\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_sse_line(line: bytes) -> Optional[dict]:\n",
    "        \"\"\"è§£æå•è¡Œ SSE æ•°æ®\"\"\"\n",
    "        if not line:\n",
    "            return None\n",
    "        \n",
    "        decoded = line.decode('utf-8')\n",
    "        \n",
    "        # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š\n",
    "        if not decoded or decoded.startswith(':') or not decoded.startswith('data: '):\n",
    "            return None\n",
    "        \n",
    "        data = decoded[6:]  # å»æ‰ \"data: \"\n",
    "        \n",
    "        # æ£€æŸ¥ç»“æŸæ ‡è®°\n",
    "        if data == '[DONE]':\n",
    "            return {'done': True}\n",
    "        \n",
    "        try:\n",
    "            return json.loads(data)\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_delta(chunk: dict) -> dict:\n",
    "        \"\"\"ä» chunk ä¸­æå–å¢é‡ä¿¡æ¯\"\"\"\n",
    "        result = {\n",
    "            'content': '',\n",
    "            'reasoning_content': '',\n",
    "            'role': None,\n",
    "            'finish_reason': None\n",
    "        }\n",
    "        \n",
    "        if not chunk or 'choices' not in chunk:\n",
    "            return result\n",
    "        \n",
    "        choice = chunk['choices'][0]\n",
    "        delta = choice.get('delta', {})\n",
    "        \n",
    "        result['content'] = delta.get('content', '')\n",
    "        result['reasoning_content'] = delta.get('reasoning_content', '')\n",
    "        result['role'] = delta.get('role')\n",
    "        result['finish_reason'] = choice.get('finish_reason')\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "# æµ‹è¯•è§£æå™¨\n",
    "parser = StreamingParser()\n",
    "\n",
    "# æµ‹è¯•ç”¨ä¾‹\n",
    "test_lines = [\n",
    "    b'data: {\"choices\": [{\"delta\": {\"role\": \"assistant\"}}]}',\n",
    "    b'data: {\"choices\": [{\"delta\": {\"content\": \"ä½ å¥½\"}}]}',\n",
    "    b'data: {\"choices\": [{\"delta\": {\"content\": \"ä¸–ç•Œ\"}}]}',\n",
    "    b'data: [DONE]'\n",
    "]\n",
    "\n",
    "print(\"ğŸ“‹ è§£æå™¨æµ‹è¯•:\\n\" + \"-\"*40)\n",
    "for line in test_lines:\n",
    "    parsed = parser.parse_sse_line(line)\n",
    "    if parsed:\n",
    "        if parsed.get('done'):\n",
    "            print(\"âœ… æ£€æµ‹åˆ°ç»“æŸæ ‡è®°: [DONE]\")\n",
    "        else:\n",
    "            delta = parser.extract_delta(parsed)\n",
    "            print(f\"å†…å®¹: '{delta['content']}' | æ€ç»´é“¾: '{delta['reasoning_content']}'\")\n",
    "\n",
    "print(\"\\nâœ… è§£æå™¨å·¥ä½œæ­£å¸¸\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. æ€§èƒ½å¯¹æ¯”ï¼šæµå¼ vs éæµå¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€§èƒ½å¯¹æ¯”æµ‹è¯•\n",
    "import pandas as pd\n",
    "\n",
    "test_prompt = \"è¯·è¯¦ç»†è§£é‡Šé‡å­è®¡ç®—çš„åŸºæœ¬åŸç†å’Œåº”ç”¨å‰æ™¯ï¼ˆçº¦300å­—ï¼‰\"\n",
    "messages = [{\"role\": \"user\", \"content\": test_prompt}]\n",
    "\n",
    "models = [\n",
    "    (\"glm-4.7-flash\", \"GLM-4.7-Flash\"),\n",
    "    (\"glm-4-flash-250414\", \"GLM-4-Flash-250414\"),\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"âš¡ æ€§èƒ½å¯¹æ¯”æµ‹è¯•å¼€å§‹\\n\" + \"=\"*60)\n",
    "\n",
    "for model_code, model_name in models:\n",
    "    print(f\"\\nğŸ“Š æµ‹è¯•æ¨¡å‹: {model_name}\")\n",
    "    \n",
    "    # æµå¼æµ‹è¯•\n",
    "    print(\"  ğŸŒŠ æµå¼æ¨¡å¼...\", end=\"\")\n",
    "    _, _, stream_time = stream_chat_completion(\n",
    "        model_code, messages, show_realtime=False\n",
    "    )\n",
    "    print(f\" è€—æ—¶: {stream_time:.3f}s\")\n",
    "    \n",
    "    # éæµå¼æµ‹è¯•\n",
    "    print(\"  ğŸ“¦ éæµå¼æ¨¡å¼...\", end=\"\")\n",
    "    _, _, non_stream_time = non_stream_chat_completion(model_code, messages)\n",
    "    print(f\" è€—æ—¶: {non_stream_time:.3f}s\")\n",
    "    \n",
    "    results.append({\n",
    "        'æ¨¡å‹': model_name,\n",
    "        'æµå¼è€—æ—¶(s)': round(stream_time, 3),\n",
    "        'éæµå¼è€—æ—¶(s)': round(non_stream_time, 3),\n",
    "        'å·®å¼‚(s)': round(non_stream_time - stream_time, 3)\n",
    "    })\n",
    "\n",
    "# æ˜¾ç¤ºç»“æœè¡¨æ ¼\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“ˆ æ€§èƒ½å¯¹æ¯”ç»“æœ:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(\"\\nğŸ’¡ è¯´æ˜:\")\n",
    "print(\"   - æµå¼è€—æ—¶ï¼šåŒ…å«é¦– token åˆ°ç»“æŸçš„å®Œæ•´æ—¶é—´\")\n",
    "print(\"   - éæµå¼è€—æ—¶ï¼šç­‰å¾…å®Œæ•´å“åº”çš„æ—¶é—´\")\n",
    "print(\"   - å®é™…ä½“éªŒï¼šæµå¼æ¨¡å¼é¦– token å»¶è¿Ÿæ›´ä½ï¼Œç”¨æˆ·ä½“éªŒæ›´å¥½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. é”™è¯¯å¤„ç†æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é”™è¯¯å¤„ç†æµ‹è¯•\n",
    "def test_error_handling():\n",
    "    \"\"\"æµ‹è¯•å„ç§é”™è¯¯æƒ…å†µ\"\"\"\n",
    "    \n",
    "    print(\"ğŸ§ª é”™è¯¯å¤„ç†æµ‹è¯•\\n\" + \"=\"*60)\n",
    "    \n",
    "    # æµ‹è¯• 1ï¼šæ— æ•ˆçš„ API Key\n",
    "    print(\"\\nâŒ æµ‹è¯• 1: æ— æ•ˆçš„ API Key\")\n",
    "    try:\n",
    "        _, _, _ = stream_chat_completion(\n",
    "            \"glm-4.7-flash\",\n",
    "            [{\"role\": \"user\", \"content\": \"ä½ å¥½\"}],\n",
    "            api_key=\"invalid_key\",\n",
    "            show_realtime=False\n",
    "        )\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"   âœ… æ­£ç¡®æ•è·é”™è¯¯: {e.response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ å…¶ä»–é”™è¯¯: {type(e).__name__}\")\n",
    "    \n",
    "    # æµ‹è¯• 2ï¼šæ— æ•ˆçš„æ¨¡å‹\n",
    "    print(\"\\nâŒ æµ‹è¯• 2: æ— æ•ˆçš„æ¨¡å‹ä»£ç \")\n",
    "    try:\n",
    "        _, _, _ = stream_chat_completion(\n",
    "            \"invalid-model-name\",\n",
    "            [{\"role\": \"user\", \"content\": \"ä½ å¥½\"}],\n",
    "            show_realtime=False\n",
    "        )\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"   âœ… æ­£ç¡®æ•è·é”™è¯¯: {e.response.status_code}\")\n",
    "        if e.response.status_code == 400:\n",
    "            print(\"   â„¹ï¸  400 é”™è¯¯ï¼šæ¨¡å‹ä¸å­˜åœ¨æˆ–å‚æ•°é”™è¯¯\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ å…¶ä»–é”™è¯¯: {type(e).__name__}\")\n",
    "    \n",
    "    # æµ‹è¯• 3ï¼šç©ºæ¶ˆæ¯\n",
    "    print(\"\\nâŒ æµ‹è¯• 3: ç©ºæ¶ˆæ¯\")\n",
    "    try:\n",
    "        _, _, _ = stream_chat_completion(\n",
    "            \"glm-4.7-flash\",\n",
    "            [],\n",
    "            show_realtime=False\n",
    "        )\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"   âœ… æ­£ç¡®æ•è·é”™è¯¯: {e.response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ å…¶ä»–é”™è¯¯: {type(e).__name__}: {e}\")\n",
    "    \n",
    "    # æµ‹è¯• 4ï¼šè¶…é•¿æ¶ˆæ¯\n",
    "    print(\"\\nâŒ æµ‹è¯• 4: è¶…é•¿æ¶ˆæ¯ï¼ˆè¶…å‡ºä¸Šä¸‹æ–‡é™åˆ¶ï¼‰\")\n",
    "    try:\n",
    "        long_content = \"è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•ã€‚\" * 50000  # çº¦ 15 ä¸‡å­—\n",
    "        _, _, _ = stream_chat_completion(\n",
    "            \"glm-4v-flash\",  # åªæœ‰ 16K ä¸Šä¸‹æ–‡\n",
    "            [{\"role\": \"user\", \"content\": long_content}],\n",
    "            show_realtime=False\n",
    "        )\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"   âœ… æ­£ç¡®æ•è·é”™è¯¯: {e.response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ å…¶ä»–é”™è¯¯: {type(e).__name__}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… é”™è¯¯å¤„ç†æµ‹è¯•å®Œæˆ\")\n",
    "\n",
    "test_error_handling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. å®Œæ•´æµå¼å®¢æˆ·ç«¯å°è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZhipuStreamClient:\n",
    "    \"\"\"\n",
    "    æ™ºè°± AI æµå¼è¾“å‡ºå®¢æˆ·ç«¯\n",
    "    \n",
    "    æ”¯æŒæ‰€æœ‰å…è´¹æ¨¡å‹ï¼š\n",
    "    - glm-4.7-flash: 200K ä¸Šä¸‹æ–‡ï¼Œè¶…é•¿æ–‡æœ¬\n",
    "    - glm-4-flash-250414: 128K ä¸Šä¸‹æ–‡ï¼Œè½»é‡å¿«é€Ÿ\n",
    "    - glm-4.6v-flash: è§†è§‰ç†è§£ï¼Œè‡ªåŠ¨æ€è€ƒ\n",
    "    - glm-4.1v-thinking-flash: è§†è§‰ + æ€ç»´é“¾\n",
    "    - glm-4v-flash: åŸºç¡€è§†è§‰æ¨¡å‹\n",
    "    \"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://open.bigmodel.cn/api/paas/v4/chat/completions\"\n",
    "    \n",
    "    # æ¨¡å‹é…ç½®\n",
    "    MODELS = {\n",
    "        \"glm-4.7-flash\": {\n",
    "            \"context\": \"200K\",\n",
    "            \"max_output\": \"128K\",\n",
    "            \"thinking\": False,\n",
    "            \"vision\": False,\n",
    "            \"description\": \"è¶…é•¿ä¸Šä¸‹æ–‡æ–‡æœ¬æ¨¡å‹\"\n",
    "        },\n",
    "        \"glm-4-flash-250414\": {\n",
    "            \"context\": \"128K\",\n",
    "            \"max_output\": \"16K\",\n",
    "            \"thinking\": False,\n",
    "            \"vision\": False,\n",
    "            \"description\": \"è½»é‡çº§å¿«é€Ÿæ–‡æœ¬æ¨¡å‹\"\n",
    "        },\n",
    "        \"glm-4.6v-flash\": {\n",
    "            \"context\": \"128K\",\n",
    "            \"max_output\": \"32K\",\n",
    "            \"thinking\": \"auto\",\n",
    "            \"vision\": True,\n",
    "            \"description\": \"è§†è§‰ç†è§£æ¨¡å‹ï¼ˆè‡ªåŠ¨æ€è€ƒï¼‰\"\n",
    "        },\n",
    "        \"glm-4.1v-thinking-flash\": {\n",
    "            \"context\": \"64K\",\n",
    "            \"max_output\": \"16K\",\n",
    "            \"thinking\": True,\n",
    "            \"vision\": True,\n",
    "            \"description\": \"è§†è§‰æ€è€ƒæ¨¡å‹ï¼ˆæ”¯æŒæ€ç»´é“¾ï¼‰\"\n",
    "        },\n",
    "        \"glm-4v-flash\": {\n",
    "            \"context\": \"16K\",\n",
    "            \"max_output\": \"1K\",\n",
    "            \"thinking\": False,\n",
    "            \"vision\": True,\n",
    "            \"description\": \"åŸºç¡€è§†è§‰æ¨¡å‹\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self, api_key: str = API_KEY):\n",
    "        self.api_key = api_key\n",
    "        self.session = requests.Session()\n",
    "    \n",
    "    def chat(\n",
    "        self,\n",
    "        model: str,\n",
    "        content: str,\n",
    "        image_url: str = None,\n",
    "        show_stream: bool = True,\n",
    "        show_reasoning: bool = True\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        æµå¼å¯¹è¯\n",
    "        \n",
    "        Args:\n",
    "            model: æ¨¡å‹ä»£ç \n",
    "            content: æ–‡æœ¬å†…å®¹\n",
    "            image_url: å›¾ç‰‡ URLï¼ˆè§†è§‰æ¨¡å‹ä½¿ç”¨ï¼‰\n",
    "            show_stream: æ˜¯å¦å®æ—¶æ˜¾ç¤ºè¾“å‡º\n",
    "            show_reasoning: æ˜¯å¦æ˜¾ç¤ºæ€ç»´é“¾\n",
    "        \n",
    "        Returns:\n",
    "            dict: {content, reasoning, duration, first_token_time}\n",
    "        \"\"\"\n",
    "        # æ„å»ºæ¶ˆæ¯\n",
    "        if image_url:\n",
    "            messages = [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n",
    "                    {\"type\": \"text\", \"text\": content}\n",
    "                ]\n",
    "            }]\n",
    "        else:\n",
    "            messages = [{\"role\": \"user\", \"content\": content}]\n",
    "        \n",
    "        # å‘é€è¯·æ±‚\n",
    "        start_time = time.time()\n",
    "        first_token_time = None\n",
    "        \n",
    "        response = self.session.post(\n",
    "            url=self.BASE_URL,\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            },\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"messages\": messages,\n",
    "                \"stream\": True\n",
    "            },\n",
    "            stream=True,\n",
    "            timeout=(10, 300)\n",
    "        )\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # è§£ææµå¼å“åº”\n",
    "        full_content = []\n",
    "        full_reasoning = []\n",
    "        \n",
    "        if show_stream:\n",
    "            model_info = self.MODELS.get(model, {})\n",
    "            thinking_support = model_info.get('thinking')\n",
    "            print(f\"\\nğŸ¤– [{model}]\")\n",
    "            print(f\"   {model_info.get('description', '')}\")\n",
    "            if thinking_support:\n",
    "                print(f\"   ğŸ’­ æ€ç»´é“¾: {'æ”¯æŒ' if thinking_support == True else 'è‡ªåŠ¨'}\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        for line in response.iter_lines():\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            parsed = StreamingParser.parse_sse_line(line)\n",
    "            if not parsed:\n",
    "                continue\n",
    "            \n",
    "            if parsed.get('done'):\n",
    "                break\n",
    "            \n",
    "            delta = StreamingParser.extract_delta(parsed)\n",
    "            \n",
    "            if first_token_time is None and delta['content']:\n",
    "                first_token_time = time.time() - start_time\n",
    "            \n",
    "            if delta['content']:\n",
    "                full_content.append(delta['content'])\n",
    "                if show_stream:\n",
    "                    print(delta['content'], end='', flush=True)\n",
    "            \n",
    "            if delta['reasoning_content']:\n",
    "                full_reasoning.append(delta['reasoning_content'])\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        result_content = ''.join(full_content)\n",
    "        result_reasoning = ''.join(full_reasoning) if full_reasoning else None\n",
    "        \n",
    "        if show_stream:\n",
    "            print(f\"\\n\\n\" + \"-\" * 50)\n",
    "            print(f\"â±ï¸ é¦– token: {first_token_time:.3f}s | æ€»è€—æ—¶: {duration:.3f}s\")\n",
    "            if result_reasoning and show_reasoning:\n",
    "                print(f\"\\nğŸ§  æ€ç»´é“¾:\\n{result_reasoning}\")\n",
    "        \n",
    "        return {\n",
    "            'content': result_content,\n",
    "            'reasoning': result_reasoning,\n",
    "            'duration': duration,\n",
    "            'first_token_time': first_token_time\n",
    "        }\n",
    "    \n",
    "    def list_models(self):\n",
    "        \"\"\"æ˜¾ç¤ºæ‰€æœ‰æ”¯æŒçš„æ¨¡å‹\"\"\"\n",
    "        print(\"ğŸ“‹ æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨:\\n\" + \"=\" * 60)\n",
    "        for code, info in self.MODELS.items():\n",
    "            print(f\"\\nğŸ”¹ {code}\")\n",
    "            print(f\"   æè¿°: {info['description']}\")\n",
    "            print(f\"   ä¸Šä¸‹æ–‡: {info['context']} | æœ€å¤§è¾“å‡º: {info['max_output']}\")\n",
    "            print(f\"   è§†è§‰: {'âœ…' if info['vision'] else 'âŒ'} | æ€è€ƒ: {'âœ…' if info['thinking'] else 'âŒ' if info['thinking'] == False else 'ğŸ¤–'}\")\n",
    "\n",
    "\n",
    "# åˆå§‹åŒ–å®¢æˆ·ç«¯\n",
    "client = ZhipuStreamClient()\n",
    "client.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨å®¢æˆ·ç«¯æµ‹è¯•\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ§ª å®¢æˆ·ç«¯åŠŸèƒ½æµ‹è¯•\\n\")\n",
    "\n",
    "# æ–‡æœ¬æ¨¡å‹\n",
    "result = client.chat(\n",
    "    model=\"glm-4.7-flash\",\n",
    "    content=\"è¯·ç”¨ä¸€å¥è¯æ€»ç»“äººå·¥æ™ºèƒ½çš„å‘å±•è¶‹åŠ¿\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€è€ƒæ¨¡å‹\n",
    "result = client.chat(\n",
    "    model=\"glm-4.1v-thinking-flash\",\n",
    "    content=\"è®¡ç®— 99 * 99ï¼Œå¹¶å±•ç¤ºæ€è€ƒè¿‡ç¨‹\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## æ€»ç»“\n",
    "\n",
    "æœ¬ Notebook å®Œæ•´æ¼”ç¤ºäº†æ™ºè°± AI æµå¼è¾“å‡ºåŠŸèƒ½ï¼š\n",
    "\n",
    "### âœ… å·²å®Œæˆæµ‹è¯•\n",
    "1. **GLM-4.7-Flash** - è¶…é•¿ä¸Šä¸‹æ–‡æ–‡æœ¬ç”Ÿæˆ\n",
    "2. **GLM-4-Flash-250414** - è½»é‡çº§å¿«é€Ÿå“åº”\n",
    "3. **GLM-4.6V-Flash** - è§†è§‰ç†è§£èƒ½åŠ›\n",
    "4. **GLM-4.1V-Thinking-Flash** - æ€ç»´é“¾å±•ç¤º\n",
    "5. **SSE è§£æå°è£…** - å®Œæ•´çš„æµå¼å“åº”å¤„ç†\n",
    "6. **æ€§èƒ½å¯¹æ¯”** - æµå¼ vs éæµå¼\n",
    "7. **é”™è¯¯å¤„ç†** - å„ç§å¼‚å¸¸æƒ…å†µå¤„ç†\n",
    "\n",
    "### ğŸ”‘ å…³é”®è¦ç‚¹\n",
    "- ä½¿ç”¨ `stream=true` å¯ç”¨æµå¼è¾“å‡º\n",
    "- SSE æ•°æ®ä»¥ `data: ` å¼€å¤´ï¼Œä»¥ `data: [DONE]` ç»“æŸ\n",
    "- è§£æå¢é‡å†…å®¹ä½¿ç”¨ `choices[0].delta.content`\n",
    "- Thinking æ¨¡å‹é¢å¤–è¿”å› `reasoning_content`\n",
    "- æµå¼è¾“å‡ºé¦– token å»¶è¿Ÿä½ï¼Œç”¨æˆ·ä½“éªŒæ›´å¥½"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
