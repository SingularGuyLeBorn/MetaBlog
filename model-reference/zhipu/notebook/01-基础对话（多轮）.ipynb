{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01-åŸºç¡€å¯¹è¯ï¼ˆå¤šè½®ï¼‰æµ‹è¯•\n",
    "\n",
    "## GLM-4.7-Flash æ¨¡å‹èƒ½åŠ›\n",
    "\n",
    "æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼ŒGLM-4.7-Flash å…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š\n",
    "\n",
    "| èƒ½åŠ› | æ”¯æŒæƒ…å†µ | è¯´æ˜ |\n",
    "|:---|:---:|:---|\n",
    "| **æ€è€ƒæ¨¡å¼** | âœ… æ”¯æŒ | æä¾›å¤šç§æ€è€ƒæ¨¡å¼ï¼Œè¦†ç›–ä¸åŒä»»åŠ¡éœ€æ±‚ |\n",
    "| **æµå¼è¾“å‡º** | âœ… æ”¯æŒ | æ”¯æŒå®æ—¶æµå¼å“åº”ï¼Œæå‡ç”¨æˆ·äº¤äº’ä½“éªŒ |\n",
    "| **Function Call** | âœ… æ”¯æŒ | å¼ºå¤§çš„å·¥å…·è°ƒç”¨èƒ½åŠ› |\n",
    "| **ä¸Šä¸‹æ–‡ç¼“å­˜** | âœ… æ”¯æŒ | æ™ºèƒ½ç¼“å­˜æœºåˆ¶ï¼Œä¼˜åŒ–é•¿å¯¹è¯æ€§èƒ½ |\n",
    "| **ç»“æ„åŒ–è¾“å‡º** | âœ… æ”¯æŒ | æ”¯æŒ JSON ç­‰ç»“æ„åŒ–æ ¼å¼è¾“å‡º |\n",
    "| **MCP** | âœ… æ”¯æŒ | å¯çµæ´»è°ƒç”¨å¤–éƒ¨ MCP å·¥å…· |\n",
    "\n",
    "## æ¨¡å‹è§„æ ¼\n",
    "\n",
    "| è§„æ ¼é¡¹ | æ•°å€¼ |\n",
    "|:---|:---|\n",
    "| è¾“å…¥æ¨¡æ€ | æ–‡æœ¬ |\n",
    "| è¾“å‡ºæ¨¡æ€ | æ–‡æœ¬ |\n",
    "| ä¸Šä¸‹æ–‡çª—å£ | 200K |\n",
    "| æœ€å¤§è¾“å‡º Tokens | 128K |\n",
    "\n",
    "## æµ‹è¯•å†…å®¹\n",
    "1. ç¯å¢ƒé…ç½®\n",
    "2. åŸºç¡€å¯¹è¯æµ‹è¯•\n",
    "3. å¤šè½®å¯¹è¯æµ‹è¯•\n",
    "4. æ€è€ƒæ¨¡å¼æµ‹è¯•ï¼ˆthinking + reasoning_contentï¼‰\n",
    "5. æµå¼è¾“å‡ºæµ‹è¯•\n",
    "6. é”™è¯¯å¤„ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒé…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…ä¾èµ–\n",
    "# !pip install openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼\n",
      "API Key: f7ec61a785...lTCk9GHCXM\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv('VITE_ZHIPU_API_KEY')\n",
    "except:\n",
    "    api_key = None\n",
    "\n",
    "# å¦‚æœç¯å¢ƒå˜é‡ä¸­æ²¡æœ‰ï¼Œä½¿ç”¨é»˜è®¤ API Key\n",
    "if not api_key:\n",
    "    api_key = \"f7ec61a7856a4f4389e037eb588c69df.0pZgcFlTCk9GHCXM\"\n",
    "\n",
    "BASE_URL = \"https://open.bigmodel.cn/api/paas/v4\"\n",
    "\n",
    "# åˆå§‹åŒ–å®¢æˆ·ç«¯\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=BASE_URL\n",
    ")\n",
    "\n",
    "print(\"âœ… å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼\")\n",
    "print(f\"API Key: {api_key[:10]}...{api_key[-10:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. åŸºç¡€å¯¹è¯æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– æ¨¡å‹å›å¤ï¼š\n",
      "\n",
      "ä½ å¥½ï¼æˆ‘æ˜¯ç”±æ™ºè°±AIè®­ç»ƒçš„GLMå¤§è¯­è¨€æ¨¡å‹ï¼Œæ˜¯ä¸€æ¬¾äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚æˆ‘èƒ½å¤Ÿç†è§£å¹¶å›åº”ä¸­æ–‡ï¼ˆä»¥åŠå…¶ä»–å¤šç§è¯­è¨€ï¼‰çš„æé—®ï¼Œå¸®åŠ©ä½ è§£ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯ã€è¿›è¡Œå¯¹è¯ç­‰ã€‚æˆ‘ä¸€ç›´åœ¨ä¸æ–­å­¦ä¹ å’Œä¼˜åŒ–ï¼Œä»¥æ›´å¥½åœ°ä¸ºç”¨æˆ·æä¾›å¸®åŠ©ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥ä¸ºä½ è§£ç­”çš„é—®é¢˜å—ï¼Ÿ\n",
      "\n",
      "ğŸ“Š Token ä½¿ç”¨ï¼š215 (è¾“å…¥: 11, è¾“å‡º: 204)\n"
     ]
    }
   ],
   "source": [
    "# åŸºç¡€å¯¹è¯ - ç®€å•é—®å€™\n",
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4.6v-flash\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"ğŸ¤– æ¨¡å‹å›å¤ï¼š\")\n",
    "print(response.choices[0].message.content)\n",
    "print(f\"\\nğŸ“Š Token ä½¿ç”¨ï¼š{response.usage.total_tokens} (è¾“å…¥: {response.usage.prompt_tokens}, è¾“å‡º: {response.usage.completion_tokens})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'code': '1302', 'message': 'æ‚¨çš„è´¦æˆ·å·²è¾¾åˆ°é€Ÿç‡é™åˆ¶ï¼Œè¯·æ‚¨æ§åˆ¶è¯·æ±‚é¢‘ç‡'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# åŸºç¡€å¯¹è¯ - å¸¸è¯†é—®ç­”\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mglm-4.7-flash\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mè¯·ç®€è¦ä»‹ç»äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ¤– æ¨¡å‹å›å¤ï¼š\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[1;32md:\\python-envs\\baseenv\\Lib\\site-packages\\openai\\_utils\\_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python-envs\\baseenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:850\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    847\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    848\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    849\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 850\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python-envs\\baseenv\\Lib\\site-packages\\openai\\_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1271\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1278\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1280\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1281\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1282\u001b[0m     )\n\u001b[1;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\python-envs\\baseenv\\Lib\\site-packages\\openai\\_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python-envs\\baseenv\\Lib\\site-packages\\openai\\_base_client.py:1049\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1048\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32md:\\python-envs\\baseenv\\Lib\\site-packages\\openai\\_base_client.py:1098\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python-envs\\baseenv\\Lib\\site-packages\\openai\\_base_client.py:1049\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1048\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32md:\\python-envs\\baseenv\\Lib\\site-packages\\openai\\_base_client.py:1098\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python-envs\\baseenv\\Lib\\site-packages\\openai\\_base_client.py:1064\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1061\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1063\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1064\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1067\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1068\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1072\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1073\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'code': '1302', 'message': 'æ‚¨çš„è´¦æˆ·å·²è¾¾åˆ°é€Ÿç‡é™åˆ¶ï¼Œè¯·æ‚¨æ§åˆ¶è¯·æ±‚é¢‘ç‡'}}"
     ]
    }
   ],
   "source": [
    "# åŸºç¡€å¯¹è¯ - å¸¸è¯†é—®ç­”\n",
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4.7-flash\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"è¯·ç®€è¦ä»‹ç»äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹\"}\n",
    "    ],\n",
    "    max_tokens=2048,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"ğŸ¤– æ¨¡å‹å›å¤ï¼š\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºç¡€å¯¹è¯ - ä»£ç ç”Ÿæˆï¼ˆAgentic Coding åœºæ™¯ï¼‰\n",
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4.7-flash\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonç¼–ç¨‹åŠ©æ‰‹ï¼Œæ“…é•¿ç¼–å†™æ¸…æ™°ã€é«˜æ•ˆçš„ä»£ç \"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"è¯·å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•ï¼Œå¹¶æ·»åŠ è¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Šï¼Œè§£é‡Šæ¯ä¸€è¡Œä»£ç çš„ä½œç”¨\"\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=4096,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"ğŸ¤– ä»£ç ç”Ÿæˆç»“æœï¼š\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. å¤šè½®å¯¹è¯æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤šè½®å¯¹è¯ä¼šè¯ç®¡ç†ç±»\n",
    "class ChatSession:\n",
    "    \"\"\"\n",
    "    å¤šè½®å¯¹è¯ä¼šè¯ç®¡ç†\n",
    "    ç»´æŠ¤å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œæ”¯æŒç³»ç»Ÿæç¤ºè¯\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, client, model, system_prompt=None, **default_params):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.messages = []\n",
    "        self.default_params = default_params\n",
    "        \n",
    "        if system_prompt:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "    def send(self, user_message, **kwargs):\n",
    "        \"\"\"\n",
    "        å‘é€æ¶ˆæ¯å¹¶è·å–å›å¤\n",
    "        \n",
    "        Args:\n",
    "            user_message: ç”¨æˆ·æ¶ˆæ¯\n",
    "            **kwargs: é¢å¤–çš„è¯·æ±‚å‚æ•°ï¼ˆè¦†ç›–é»˜è®¤å‚æ•°ï¼‰\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (assistant_message, response)\n",
    "        \"\"\"\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "        # åˆå¹¶å‚æ•°\n",
    "        params = {**self.default_params, **kwargs}\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=self.messages,\n",
    "            **params\n",
    "        )\n",
    "        \n",
    "        assistant_message = response.choices[0].message\n",
    "        self.messages.append({\n",
    "            \"role\": \"assistant\", \n",
    "            \"content\": assistant_message.content\n",
    "        })\n",
    "        \n",
    "        return assistant_message, response\n",
    "    \n",
    "    def get_history(self):\n",
    "        \"\"\"è·å–å¯¹è¯å†å²\"\"\"\n",
    "        return self.messages.copy()\n",
    "    \n",
    "    def clear(self, keep_system=True):\n",
    "        \"\"\"æ¸…ç©ºå¯¹è¯å†å²\"\"\"\n",
    "        if keep_system:\n",
    "            self.messages = [m for m in self.messages if m[\"role\"] == \"system\"]\n",
    "        else:\n",
    "            self.messages = []\n",
    "        print(\"âœ… å¯¹è¯å†å²å·²æ¸…ç©º\")\n",
    "\n",
    "print(\"âœ… ChatSession ç±»å·²å®šä¹‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºå¤šè½®å¯¹è¯ä¼šè¯\n",
    "session = ChatSession(\n",
    "    client=client,\n",
    "    model=\"glm-4.7-flash\",\n",
    "    system_prompt=\"ä½ æ˜¯ä¸€ä¸ªPythonç¼–ç¨‹ä¸“å®¶ï¼Œæ“…é•¿ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€è§£é‡Šç¼–ç¨‹æ¦‚å¿µ\",\n",
    "    max_tokens=2048,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"âœ… å¤šè½®å¯¹è¯ä¼šè¯å·²åˆ›å»º\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¬¬1è½®ï¼šè¯¢é—®é€’å½’æ¦‚å¿µ\n",
    "print(\"ğŸ“ ç¬¬1è½®ï¼šä»€ä¹ˆæ˜¯é€’å½’ï¼Ÿ\\n\")\n",
    "msg, resp = session.send(\"ä»€ä¹ˆæ˜¯é€’å½’ï¼Ÿè¯·ç”¨ç®€å•çš„è¯­è¨€è§£é‡Š\")\n",
    "print(f\"ğŸ¤– åŠ©æ‰‹ï¼š{msg.content}\\n\")\n",
    "print(f\"ğŸ“Š Tokenä½¿ç”¨ï¼š{resp.usage.total_tokens}\\n\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¬¬2è½®ï¼šè¦æ±‚ç¤ºä¾‹ï¼ˆæµ‹è¯•ä¸Šä¸‹æ–‡ä¿æŒï¼‰\n",
    "print(\"ğŸ“ ç¬¬2è½®ï¼šèƒ½ç»™æˆ‘ä¸€ä¸ªPythoné€’å½’çš„ä¾‹å­å—ï¼Ÿ\\n\")\n",
    "msg, resp = session.send(\"èƒ½ç»™æˆ‘ä¸€ä¸ªPythoné€’å½’çš„ä¾‹å­å—ï¼Ÿ\")\n",
    "print(f\"ğŸ¤– åŠ©æ‰‹ï¼š{msg.content}\\n\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¬¬3è½®ï¼šè¿½é—®ä¼˜åŒ–ï¼ˆæµ‹è¯•ä¸Šä¸‹æ–‡ç†è§£ï¼‰\n",
    "print(\"ğŸ“ ç¬¬3è½®ï¼šè¿™ä¸ªä¾‹å­çš„æ—¶é—´å¤æ‚åº¦æ˜¯å¤šå°‘ï¼Ÿå¦‚ä½•ä¼˜åŒ–ï¼Ÿ\\n\")\n",
    "msg, resp = session.send(\"è¿™ä¸ªä¾‹å­çš„æ—¶é—´å¤æ‚åº¦æ˜¯å¤šå°‘ï¼Ÿå¦‚ä½•ä¼˜åŒ–ï¼Ÿ\")\n",
    "print(f\"ğŸ¤– åŠ©æ‰‹ï¼š{msg.content}\\n\")\n",
    "print(f\"ğŸ’¬ å½“å‰å¯¹è¯è½®æ•°ï¼š{len([m for m in session.messages if m['role'] == 'user'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ€è€ƒæ¨¡å¼æµ‹è¯•\n",
    "\n",
    "GLM-4.7-Flash æ”¯æŒé€šè¿‡ `thinking` å‚æ•°å¯ç”¨æ€è€ƒæ¨¡å¼ï¼Œæ¨¡å‹ä¼šè¿”å› `reasoning_content`ï¼ˆæ€ç»´é“¾ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•ï¼šå¯ç”¨æ€è€ƒæ¨¡å¼\n",
    "print(\"ğŸ§  æµ‹è¯•æ€è€ƒæ¨¡å¼ - æ•°å­¦æ¨ç†é—®é¢˜\\n\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4.7-flash\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"å¦‚æœ 3 ä¸ªå·¥äºº 3 å°æ—¶å¯ä»¥å»º 3 é¢å¢™ï¼Œé‚£ä¹ˆ 6 ä¸ªå·¥äºº 6 å°æ—¶å¯ä»¥å»ºå‡ é¢å¢™ï¼Ÿè¯·è¯¦ç»†è§£é‡Šæ¨ç†è¿‡ç¨‹ã€‚\"\n",
    "        }\n",
    "    ],\n",
    "    thinking={\n",
    "        \"type\": \"enabled\"  # å¯ç”¨æ€è€ƒæ¨¡å¼\n",
    "    },\n",
    "    max_tokens=4096\n",
    ")\n",
    "\n",
    "message = response.choices[0].message\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦æœ‰æ€ç»´é“¾\n",
    "if hasattr(message, 'reasoning_content') and message.reasoning_content:\n",
    "    print(\"ğŸ§  æ€ç»´é“¾ï¼ˆæ¨ç†è¿‡ç¨‹ï¼‰ï¼š\")\n",
    "    print(\"-\" * 60)\n",
    "    print(message.reasoning_content)\n",
    "    print(\"-\" * 60)\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"ğŸ’¡ æœ€ç»ˆç­”æ¡ˆï¼š\")\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¹æ¯”ï¼šå…³é—­æ€è€ƒæ¨¡å¼\n",
    "print(\"ğŸ”• å…³é—­æ€è€ƒæ¨¡å¼å¯¹æ¯”\\n\")\n",
    "\n",
    "response_normal = client.chat.completions.create(\n",
    "    model=\"glm-4.7-flash\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"å¦‚æœ 3 ä¸ªå·¥äºº 3 å°æ—¶å¯ä»¥å»º 3 é¢å¢™ï¼Œé‚£ä¹ˆ 6 ä¸ªå·¥äºº 6 å°æ—¶å¯ä»¥å»ºå‡ é¢å¢™ï¼Ÿ\"\n",
    "        }\n",
    "    ],\n",
    "    thinking={\n",
    "        \"type\": \"disabled\"  # å…³é—­æ€è€ƒæ¨¡å¼\n",
    "    },\n",
    "    max_tokens=2048\n",
    ")\n",
    "\n",
    "print(\"ğŸ’¡ å…³é—­æ€è€ƒæ¨¡å¼åçš„å›ç­”ï¼š\")\n",
    "print(response_normal.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æµå¼è¾“å‡ºæµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµå¼è¾“å‡ºæµ‹è¯•\n",
    "print(\"ğŸŒŠ æµå¼è¾“å‡ºæµ‹è¯•\\n\")\n",
    "print(\"ğŸ¤– åŠ©æ‰‹ï¼š\", end=\"\", flush=True)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4.7-flash\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„çŸ­è¯—\"\n",
    "        }\n",
    "    ],\n",
    "    stream=True,  # å¯ç”¨æµå¼è¾“å‡º\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "full_content = \"\"\n",
    "\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        print(content, end=\"\", flush=True)\n",
    "        full_content += content\n",
    "\n",
    "print(\"\\n\\nâœ… æµå¼è¾“å‡ºå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµå¼è¾“å‡º + æ€è€ƒæ¨¡å¼\n",
    "print(\"ğŸŒŠğŸ§  æµå¼è¾“å‡º + æ€è€ƒæ¨¡å¼\\n\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4.7-flash\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"è¯·è§£é‡Šï¼šä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„ï¼Ÿ\"\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    "    extra_body={\"thinking\": {\"type\": \"enabled\"}},\n",
    "    max_tokens=2048\n",
    ")\n",
    "\n",
    "print(\"ğŸ§  æ€ç»´é“¾ï¼š\")\n",
    "print(\"-\" * 60)\n",
    "reasoning_parts = []\n",
    "content_parts = []\n",
    "\n",
    "for chunk in response:\n",
    "    delta = chunk.choices[0].delta\n",
    "    \n",
    "    # è·å–æ€ç»´é“¾\n",
    "    if hasattr(delta, 'reasoning_content') and delta.reasoning_content:\n",
    "        print(delta.reasoning_content, end=\"\", flush=True)\n",
    "        reasoning_parts.append(delta.reasoning_content)\n",
    "    \n",
    "    # è·å–æœ€ç»ˆå†…å®¹\n",
    "    if delta.content:\n",
    "        content_parts.append(delta.content)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"\\nğŸ’¡ æœ€ç»ˆç­”æ¡ˆï¼š\")\n",
    "print(\"\".join(content_parts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. é”™è¯¯å¤„ç†æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•1ï¼šæ— æ•ˆ API Key\n",
    "try:\n",
    "    bad_client = OpenAI(api_key=\"invalid-key\", base_url=BASE_URL)\n",
    "    bad_client.chat.completions.create(\n",
    "        model=\"glm-4.7-flash\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"test\"}]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"âœ… æ— æ•ˆ API Key é”™è¯¯æ•è·ï¼š{type(e).__name__}\")\n",
    "    print(f\"   é”™è¯¯ï¼š{str(e)[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•2ï¼šæ— æ•ˆæ¨¡å‹åç§°\n",
    "try:\n",
    "    client.chat.completions.create(\n",
    "        model=\"invalid-model-name\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"test\"}]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"âœ… æ— æ•ˆæ¨¡å‹é”™è¯¯æ•è·ï¼š{type(e).__name__}\")\n",
    "    print(f\"   é”™è¯¯ï¼š{str(e)[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•3ï¼šç©ºæ¶ˆæ¯åˆ—è¡¨\n",
    "try:\n",
    "    client.chat.completions.create(\n",
    "        model=\"glm-4.7-flash\",\n",
    "        messages=[]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"âœ… ç©ºæ¶ˆæ¯é”™è¯¯æ•è·ï¼š{type(e).__name__}\")\n",
    "    print(f\"   é”™è¯¯ï¼š{str(e)[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è¶…é•¿è¾“å…¥é”™è¯¯æ•è·ï¼šRateLimitError\n",
      "   é”™è¯¯ï¼šError code: 429 - {'error': {'code': '1302', 'message': 'æ‚¨çš„è´¦æˆ·å·²è¾¾åˆ°é€Ÿç‡é™åˆ¶ï¼Œè¯·æ‚¨æ§åˆ¶è¯·æ±‚é¢‘ç‡'}}...\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯•4ï¼šè¶…é•¿è¾“å…¥\n",
    "try:\n",
    "    long_text = \"è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•ã€‚\" * 50000  # çº¦10ä¸‡å­—\n",
    "    client.chat.completions.create(\n",
    "        model=\"glm-4.7-flash\",\n",
    "        messages=[{\"role\": \"user\", \"content\": long_text}]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"âœ… è¶…é•¿è¾“å…¥é”™è¯¯æ•è·ï¼š{type(e).__name__}\")\n",
    "    print(f\"   é”™è¯¯ï¼š{str(e)[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. æ€»ç»“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    GLM-4.7-Flash æµ‹è¯•æ€»ç»“                      â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘ æ¨¡å‹è§„æ ¼ï¼š                                                      â•‘\n",
    "â•‘   â€¢ è¾“å…¥æ¨¡æ€ï¼šæ–‡æœ¬                                              â•‘\n",
    "â•‘   â€¢ è¾“å‡ºæ¨¡æ€ï¼šæ–‡æœ¬                                              â•‘\n",
    "â•‘   â€¢ ä¸Šä¸‹æ–‡çª—å£ï¼š200K tokens                                     â•‘\n",
    "â•‘   â€¢ æœ€å¤§è¾“å‡ºï¼š128K tokens                                       â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘ èƒ½åŠ›æ”¯æŒï¼š                                                      â•‘\n",
    "â•‘   âœ… æ€è€ƒæ¨¡å¼ - æ”¯æŒ reasoning_contentï¼ˆæ€ç»´é“¾ï¼‰                 â•‘\n",
    "â•‘   âœ… æµå¼è¾“å‡º - å®æ—¶å“åº”                                        â•‘\n",
    "â•‘   âœ… Function Call - å·¥å…·è°ƒç”¨                                   â•‘\n",
    "â•‘   âœ… ä¸Šä¸‹æ–‡ç¼“å­˜ - é•¿å¯¹è¯ä¼˜åŒ–                                     â•‘\n",
    "â•‘   âœ… ç»“æ„åŒ–è¾“å‡º - JSON æ ¼å¼                                     â•‘\n",
    "â•‘   âœ… MCP - å¤–éƒ¨å·¥å…·è°ƒç”¨                                         â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘ ä½¿ç”¨å»ºè®®ï¼š                                                      â•‘\n",
    "â•‘   â€¢ éœ€è¦æ·±åº¦æ¨ç† â†’ å¯ç”¨ thinking={type: enabled}                â•‘\n",
    "â•‘   â€¢ å®æ—¶äº¤äº’åœºæ™¯ â†’ ä½¿ç”¨ stream=true                             â•‘\n",
    "â•‘   â€¢ é•¿æ–‡æ¡£å¤„ç† â†’ åˆ©ç”¨ 200K ä¸Šä¸‹æ–‡                               â•‘\n",
    "â•‘   â€¢ ä»£ç ç”Ÿæˆ â†’ é€‚åˆ Agentic Coding åœºæ™¯                         â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseenv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
