# 01-基础对话（多轮）

## GLM-4.7-Flash 模型

### 模型概述

GLM-4.7-Flash 是智谱推出的 30B 级 SOTA 模型，提供一个兼顾性能与效率的新选择。面向 **Agentic Coding** 场景强化了编码能力、长程任务规划与工具协同。

### 模型规格

| 规格项 | 数值 |
|:---|:---|
| 输入模态 | 文本 |
| 输出模态 | 文本 |
| 上下文窗口 | 200K tokens |
| 最大输出 Tokens | 128K |
| 思考模式 | ✅ 支持（可选） |
| 思维链 | ✅ 支持 |

### 能力支持

- ✅ **思考模式** - 提供多种思考模式，覆盖不同任务需求
- ✅ **流式输出** - 支持实时流式响应
- ✅ **Function Call** - 强大的工具调用能力
- ✅ **上下文缓存** - 智能缓存机制，优化长对话性能
- ✅ **结构化输出** - 支持 JSON 等结构化格式
- ✅ **MCP** - 可灵活调用外部 MCP 工具

### 适用场景

1. **Agentic Coding** - 从目标描述出发，自主完成需求理解、方案拆解与多技术栈整合
2. **多模态交互与实时应用开发** - 摄像头、实时输入与交互控制
3. **前端视觉审美优化** - 布局结构、配色和谐度与组件样式
4. **高质量对话与复杂问题协作** - 多轮对话保持上下文与约束条件
5. **沉浸式写作与角色驱动创作** - 细腻的文字表达与角色扮演
6. **专业级 PPT/海报生成** - 版式遵循与审美稳定性
7. **智能搜索与 Deep Research** - 复杂问题与研究型任务

### API 调用方式

**端点**：`https://open.bigmodel.cn/api/paas/v4/chat/completions`

**请求方式**：POST

**认证方式**：Bearer Token

### 基础调用示例

```bash
curl -X POST "https://open.bigmodel.cn/api/paas/v4/chat/completions" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer your-api-key" \
    -d '{
        "model": "glm-4.7-flash",
        "messages": [
            {"role": "user", "content": "你好，请介绍一下自己"}
        ],
        "max_tokens": 2048,
        "temperature": 0.7
    }'
```

### Python 调用示例

```python
from openai import OpenAI

client = OpenAI(
    api_key="your-api-key",
    base_url="https://open.bigmodel.cn/api/paas/v4"
)

# 基础对话
response = client.chat.completions.create(
    model="glm-4.7-flash",
    messages=[
        {"role": "user", "content": "你好，请介绍一下自己"}
    ]
)
print(response.choices[0].message.content)
```

### 思考模式调用

```python
# 启用思考模式
response = client.chat.completions.create(
    model="glm-4.7-flash",
    messages=[
        {"role": "user", "content": "请解决这道数学题：如果 3 个工人 3 小时可以建 3 面墙..."}
    ],
    thinking={"type": "enabled"},  # 启用深度思考
    max_tokens=4096
)

# 获取思维链和最终答案
message = response.choices[0].message
if hasattr(message, 'reasoning_content'):
    print("思维链：", message.reasoning_content)
print("最终答案：", message.content)
```

### 流式输出调用

```python
response = client.chat.completions.create(
    model="glm-4.7-flash",
    messages=[
        {"role": "user", "content": "请写一首关于春天的诗"}
    ],
    stream=True  # 启用流式输出
)

for chunk in response:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

### 多轮对话示例

```python
# 维护对话上下文
messages = [
    {"role": "system", "content": "你是一个Python编程助手"},
    {"role": "user", "content": "什么是递归？"},
    {"role": "assistant", "content": "递归是一种函数调用自身的编程技术..."},
    {"role": "user", "content": "能给我一个Python递归的例子吗？"}
]

response = client.chat.completions.create(
    model="glm-4.7-flash",
    messages=messages
)
```

### 参数说明

| 参数 | 类型 | 必填 | 说明 |
|:---|:---|:---:|:---|
| model | string | 是 | 模型代码：`glm-4.7-flash` |
| messages | array | 是 | 对话消息列表 |
| max_tokens | integer | 否 | 最大输出 tokens，默认 2048 |
| temperature | float | 否 | 采样温度，0-1，默认 0.7 |
| stream | boolean | 否 | 是否启用流式输出，默认 false |
| thinking | object | 否 | 思考模式配置 `{type: "enabled\|disabled"}` |

### 注意事项

1. **思考模式**：可通过 `thinking` 参数开启/关闭，开启后模型会返回 `reasoning_content`（思维链）
2. **流式输出**：启用 `stream=true` 时，需要正确处理 SSE 流
3. **上下文长度**：最大支持 200K tokens，适合长文档处理
4. **温度设置**：创意写作建议 0.7-1.0，代码生成建议 0.2-0.5

### 与 GLM-4-Flash-250414 的区别

| 能力 | GLM-4.7-Flash | GLM-4-Flash-250414 |
|:---|:---:|:---:|
| 思考模式 | ✅ 支持 | ❌ 不支持 |
| 上下文 | 200K | 128K |
| 工具调用 | ✅ 支持 | ✅ 支持 |
| 最大输出 | 128K | 16K |
