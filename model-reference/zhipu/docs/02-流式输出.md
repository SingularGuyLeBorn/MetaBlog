# æµå¼è¾“å‡º (Streaming)

## åŠŸèƒ½ä»‹ç»

æµå¼è¾“å‡ºï¼ˆStreamingï¼‰é€šè¿‡ **Server-Sent Events (SSE)** æŠ€æœ¯å®æ—¶è¿”å›ç”Ÿæˆå†…å®¹ï¼Œè®©ç”¨æˆ·æ— éœ€ç­‰å¾…å®Œæ•´å“åº”å³å¯çœ‹åˆ°é€æ­¥ç”Ÿæˆçš„ç»“æœï¼Œæ˜¾è‘—æå‡äº†äº¤äº’ä½“éªŒã€‚

### å…è´¹æ¨¡å‹æ”¯æŒæƒ…å†µ

æ™ºè°± AI æä¾›ä»¥ä¸‹æ”¯æŒæµå¼è¾“å‡ºçš„å…è´¹æ¨¡å‹ï¼š

| æ¨¡å‹ | æ¨¡å‹ä»£ç  | ä¸Šä¸‹æ–‡ | æœ€å¤§è¾“å‡º | æ€è€ƒæ¨¡å¼ | æ€ç»´é“¾æ”¯æŒ |
|:---|:---|:---:|:---:|:---:|:---:|
| GLM-4.7-Flash | glm-4.7-flash | 200K | 128K | âœ… æ”¯æŒ | âœ… æ”¯æŒ |
| GLM-4-Flash-250414 | glm-4-flash-250414 | 128K | 16K | âŒ ä¸æ”¯æŒ | âŒ ä¸æ”¯æŒ |
| GLM-4.6V-Flash | glm-4.6v-flash | 128K | 32K | âœ… æ”¯æŒ | âœ… æ”¯æŒ |
| GLM-4.1V-Thinking-Flash | glm-4.1v-thinking-flash | 64K | 16K | âœ… å†…ç½® | âœ… æ”¯æŒ |
| GLM-4V-Flash | glm-4v-flash | 16K | 1K | âŒ ä¸æ”¯æŒ | âŒ ä¸æ”¯æŒ |

**è¯´æ˜ï¼š**
- **æ€è€ƒæ¨¡å¼**ï¼šæŒ‡æ¨¡å‹æ˜¯å¦æ”¯æŒæ·±åº¦æ€è€ƒæ¨¡å¼ï¼ˆthinking å‚æ•°ï¼‰
- **æ€ç»´é“¾æ”¯æŒ**ï¼šæŒ‡æ¨¡å‹æ˜¯å¦ä¼šåœ¨å“åº”ä¸­è¿”å› `reasoning_content`
- `GLM-4.7-Flash`ã€`GLM-4.6V-Flash` æ”¯æŒ `thinking` å‚æ•°æ§åˆ¶æ€è€ƒæ¨¡å¼
- `GLM-4.1V-Thinking-Flash` å¼ºåˆ¶æ€è€ƒï¼Œå§‹ç»ˆè¿”å› `reasoning_content`

---

## æµå¼è¾“å‡º vs éæµå¼è¾“å‡º

| ç‰¹æ€§ | æµå¼è¾“å‡º (stream=true) | éæµå¼è¾“å‡º (stream=false) |
|:---|:---|:---|
| **å“åº”æ–¹å¼** | é€å­—/é€å¥å®æ—¶è¿”å› | å®Œæ•´ç”Ÿæˆåä¸€æ¬¡æ€§è¿”å› |
| **é¦– token å»¶è¿Ÿ** | æä½ï¼ˆæ¯«ç§’çº§ï¼‰ | è¾ƒé«˜ï¼ˆç­‰å¾…å®Œæ•´ç”Ÿæˆï¼‰ |
| **ç”¨æˆ·ä½“éªŒ** | å³æ—¶åé¦ˆï¼Œæ„ŸçŸ¥ç”Ÿæˆè¿‡ç¨‹ | ç­‰å¾…åä¸€æ¬¡æ€§å±•ç¤º |
| **é€‚ç”¨åœºæ™¯** | èŠå¤©å¯¹è¯ã€å®æ—¶å±•ç¤º | æ‰¹é‡å¤„ç†ã€éœ€è¦å®Œæ•´ç»“æœ |
| **è¿æ¥æ–¹å¼** | SSE (Server-Sent Events) | æ ‡å‡† HTTP JSON |
| **æ•°æ®å¤„ç†** | éœ€è¦é€è¡Œè§£æ | ç›´æ¥è§£æ JSON |

---

## é€‚ç”¨åœºæ™¯

### âœ… æ¨èä½¿ç”¨æµå¼è¾“å‡º

1. **èŠå¤©å¯¹è¯åº”ç”¨** - ç”¨æˆ·è¾“å…¥åç«‹å³çœ‹åˆ° AI å¼€å§‹å›åº”
2. **é•¿æ–‡æœ¬ç”Ÿæˆ** - æ–‡ç« ã€æ•…äº‹ç­‰åˆ›ä½œæ—¶å®æ—¶å±•ç¤ºè¿›åº¦
3. **ä»£ç ç”Ÿæˆ** - ç¨‹åºå‘˜å¯ç«‹å³çœ‹åˆ°ä»£ç ç»“æ„
4. **æ•™å­¦è§£é‡Š** - é€æ­¥å±•ç¤ºè§£é¢˜è¿‡ç¨‹æˆ–è§£é‡Šæ­¥éª¤
5. **å®æ—¶äº¤äº’ç³»ç»Ÿ** - éœ€è¦å³æ—¶åé¦ˆçš„åœºæ™¯

### âŒ ä¸æ¨èä½¿ç”¨æµå¼è¾“å‡º

1. **æ‰¹é‡æ•°æ®å¤„ç†** - éœ€è¦å®Œæ•´ç»“æœè¿›è¡Œåç»­è®¡ç®—
2. **API ä»£ç†æœåŠ¡** - éœ€è¦ç¼“å­˜å®Œæ•´å“åº”
3. **ç»“æœéœ€è¦ç²¾ç¡®ç»Ÿè®¡** - å¦‚ token è®¡æ•°ã€å­—æ•°ç»Ÿè®¡
4. **ç½‘ç»œä¸ç¨³å®šç¯å¢ƒ** - SSE è¿æ¥å¯èƒ½ä¸­æ–­

---

## API è°ƒç”¨æ–¹å¼

### è¯·æ±‚å‚æ•°

```json
{
  "model": "glm-4.7-flash",
  "messages": [
    {"role": "user", "content": "ä½ å¥½"}
  ],
  "stream": true  // å¯ç”¨æµå¼è¾“å‡º
}
```

### è¯·æ±‚ç¤ºä¾‹

```python
import requests

response = requests.post(
    url="https://open.bigmodel.cn/api/paas/v4/chat/completions",
    headers={
        "Authorization": "Bearer YOUR_API_KEY",
        "Content-Type": "application/json"
    },
    json={
        "model": "glm-4.7-flash",
        "messages": [{"role": "user", "content": "è®²ä¸€ä¸ªçŸ­æ•…äº‹"}],
        "stream": true
    },
    stream=True  # å¼€å¯ requests æµå¼æ¨¡å¼
)
```

---

## SSE äº‹ä»¶å¤„ç†

### SSE æ•°æ®æ ¼å¼

æµå¼å“åº”è¿”å›çš„æ˜¯ **text/event-stream** æ ¼å¼ï¼Œæ¯è¡Œä»¥ `data: ` å¼€å¤´ï¼š

```
data: {"id":"123","choices":[{"delta":{"content":"ä½ "}}]}

data: {"id":"123","choices":[{"delta":{"content":"å¥½"}}]}

data: {"id":"123","choices":[{"delta":{"content":"ï¼"}}]}

data: [DONE]
```

### å…³é”®å­—æ®µè¯´æ˜

| å­—æ®µ | è¯´æ˜ |
|:---|:---|
| `data: [DONE]` | æµå¼è¾“å‡ºç»“æŸæ ‡è®° |
| `choices[0].delta.content` | æ–°å¢çš„æ–‡æœ¬å†…å®¹ |
| `choices[0].delta.reasoning_content` | æ€ç»´é“¾å†…å®¹ï¼ˆæ€è€ƒæ¨¡å‹ï¼‰ |
| `choices[0].finish_reason` | ç»“æŸåŸå› ï¼ˆstop/length/nullï¼‰ |

### è§£ææ­¥éª¤

```python
def parse_sse_stream(response):
    """è§£æ SSE æµå¼å“åº”"""
    for line in response.iter_lines():
        if line:
            line = line.decode('utf-8')
            # è·³è¿‡ç©ºè¡Œ
            if line.startswith('data: '):
                data = line[6:]  # å»æ‰ "data: " å‰ç¼€
                
                # æ£€æŸ¥ç»“æŸæ ‡è®°
                if data == '[DONE]':
                    break
                
                # è§£æ JSON æ•°æ®
                import json
                chunk = json.loads(data)
                
                # æå–å†…å®¹å¢é‡
                delta = chunk['choices'][0].get('delta', {})
                content = delta.get('content', '')
                reasoning = delta.get('reasoning_content', '')
                
                yield content, reasoning
```

---

## ä»£ç ç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šæ–‡æœ¬æ¨¡å‹æµå¼è¾“å‡º

```python
import requests
import json

def stream_chat_completion(model, messages, api_key):
    """æµå¼å¯¹è¯å®Œæˆ"""
    response = requests.post(
        url="https://open.bigmodel.cn/api/paas/v4/chat/completions",
        headers={
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        },
        json={
            "model": model,
            "messages": messages,
            "stream": True
        },
        stream=True
    )
    
    full_content = ""
    for line in response.iter_lines():
        if line:
            line = line.decode('utf-8')
            if line.startswith('data: '):
                data = line[6:]
                if data == '[DONE]':
                    break
                chunk = json.loads(data)
                delta = chunk['choices'][0].get('delta', {})
                content = delta.get('content', '')
                if content:
                    print(content, end='', flush=True)
                    full_content += content
    
    return full_content

# ä½¿ç”¨ç¤ºä¾‹
api_key = "YOUR_API_KEY"
messages = [{"role": "user", "content": "ç”¨ä¸‰å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½"}]
result = stream_chat_completion("glm-4.7-flash", messages, api_key)
```

### ç¤ºä¾‹ 2ï¼šè§†è§‰æ¨¡å‹æµå¼è¾“å‡º

```python
def stream_vision_chat(model, image_url, prompt, api_key):
    """è§†è§‰æ¨¡å‹æµå¼å¯¹è¯"""
    messages = [{
        "role": "user",
        "content": [
            {"type": "image_url", "image_url": {"url": image_url}},
            {"type": "text", "text": prompt}
        ]
    }]
    
    response = requests.post(
        url="https://open.bigmodel.cn/api/paas/v4/chat/completions",
        headers={
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        },
        json={
            "model": model,
            "messages": messages,
            "stream": True
        },
        stream=True
    )
    
    print("æ¨¡å‹å›å¤: ", end='', flush=True)
    for line in response.iter_lines():
        if line:
            line = line.decode('utf-8')
            if line.startswith('data: '):
                data = line[6:]
                if data == '[DONE]':
                    break
                chunk = json.loads(data)
                delta = chunk['choices'][0].get('delta', {})
                content = delta.get('content', '')
                if content:
                    print(content, end='', flush=True)

# ä½¿ç”¨ç¤ºä¾‹
stream_vision_chat(
    "glm-4.6v-flash",
    "https://example.com/image.jpg",
    "æè¿°è¿™å¼ å›¾ç‰‡",
    api_key
)
```

### ç¤ºä¾‹ 3ï¼šæ€è€ƒæ¨¡å‹æµå¼è¾“å‡ºï¼ˆå¸¦æ€ç»´é“¾ï¼‰

```python
def stream_thinking_chat(model, messages, api_key):
    """æ€è€ƒæ¨¡å‹æµå¼å¯¹è¯ï¼ˆå±•ç¤ºæ€ç»´é“¾ï¼‰"""
    response = requests.post(
        url="https://open.bigmodel.cn/api/paas/v4/chat/completions",
        headers={
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        },
        json={
            "model": model,
            "messages": messages,
            "thinking": {"type": "enabled"},  # å¯ç”¨æ€è€ƒæ¨¡å¼
            "stream": True
        },
        stream=True
    )
    
    full_content = ""
    full_reasoning = ""
    
    print("ğŸ§  æ€ç»´é“¾: ", end="", flush=True)
    for line in response.iter_lines():
        if line:
            line = line.decode('utf-8')
            if line.startswith('data: '):
                data = line[6:]
                if data == '[DONE]':
                    break
                chunk = json.loads(data)
                delta = chunk['choices'][0].get('delta', {})
                
                # æå–æ€ç»´é“¾å†…å®¹
                reasoning = delta.get('reasoning_content', '')
                if reasoning:
                    print(reasoning, end="", flush=True)
                    full_reasoning += reasoning
                
                # æå–æœ€ç»ˆå†…å®¹
                content = delta.get('content', '')
                if content:
                    full_content += content
    
    print(f"\n\nâœ… æœ€ç»ˆç­”æ¡ˆ: {full_content}")
    return full_content, full_reasoning

# ä½¿ç”¨ç¤ºä¾‹ - GLM-4.7-Flash æ”¯æŒæ€è€ƒæ¨¡å¼
messages = [{"role": "user", "content": "è§£æ–¹ç¨‹ï¼š2x + 5 = 15"}]
stream_thinking_chat("glm-4.7-flash", messages, api_key)

# GLM-4.1V-Thinking-Flash å¼ºåˆ¶æ€è€ƒï¼Œå§‹ç»ˆè¿”å›æ€ç»´é“¾
stream_thinking_chat("glm-4.1v-thinking-flash", messages, api_key)
```

---

## æ³¨æ„äº‹é¡¹

### âš ï¸ é‡è¦æç¤º

1. **ç»“æŸæ ‡è®°è¯†åˆ«**  
   SSE æµä»¥ `data: [DONE]` æ ‡è®°ç»“æŸï¼Œå¿…é¡»æ­£ç¡®å¤„ç†æ­¤æ ‡è®°ä»¥é¿å…è§£æé”™è¯¯ã€‚

2. **ç©ºå†…å®¹è¿‡æ»¤**  
   æŸäº› chunk å¯èƒ½åªåŒ…å«è§’è‰²ä¿¡æ¯ï¼ˆå¦‚ `{"role":"assistant"}`ï¼‰æˆ–ç©ºå†…å®¹ï¼Œéœ€è¿‡æ»¤å¤„ç†ã€‚

3. **ç½‘ç»œä¸­æ–­å¤„ç†**  
   æµå¼è¿æ¥å¯èƒ½å› ç½‘ç»œé—®é¢˜ä¸­æ–­ï¼Œå»ºè®®å®ç°é‡è¯•æœºåˆ¶ã€‚

4. **ç¼–ç é—®é¢˜**  
   å“åº”ä½¿ç”¨ UTF-8 ç¼–ç ï¼Œç¡®ä¿æ­£ç¡®è§£ç ä»¥é¿å…ä¹±ç ã€‚

5. **è¶…æ—¶è®¾ç½®**  
   æµå¼è¾“å‡ºå¯èƒ½æŒç»­è¾ƒé•¿æ—¶é—´ï¼Œé€‚å½“è®¾ç½®è¯·æ±‚è¶…æ—¶ï¼š
   ```python
   response = requests.post(..., stream=True, timeout=(10, 300))
   # (è¿æ¥è¶…æ—¶, è¯»å–è¶…æ—¶) å•ä½ï¼šç§’
   ```

6. **æ€§èƒ½ä¼˜åŒ–**  
   å¯¹äºé«˜é¢‘è°ƒç”¨ï¼Œå»ºè®®ä½¿ç”¨è¿æ¥æ± ï¼š
   ```python
   from requests.adapters import HTTPAdapter
   
   session = requests.Session()
   session.mount('https://', HTTPAdapter(pool_connections=10, pool_maxsize=10))
   ```

---

## å®Œæ•´å°è£…ç¤ºä¾‹

```python
import requests
import json
from typing import Iterator, Optional

class ZhipuStreamingClient:
    """æ™ºè°± AI æµå¼è¾“å‡ºå®¢æˆ·ç«¯"""
    
    BASE_URL = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.session = requests.Session()
    
    def chat_completion(
        self,
        model: str,
        messages: list,
        stream: bool = True,
        thinking: dict = None,
        **kwargs
    ) -> Iterator[dict]:
        """
        æµå¼å¯¹è¯å®Œæˆ
        
        Args:
            model: æ¨¡å‹ä»£ç 
            messages: æ¶ˆæ¯åˆ—è¡¨
            stream: æ˜¯å¦æµå¼è¾“å‡º
            thinking: æ€è€ƒæ¨¡å¼é…ç½®ï¼ˆæ”¯æŒ thinking çš„æ¨¡å‹å¯ç”¨ï¼‰
            **kwargs: å…¶ä»–å‚æ•°
        
        Yields:
            dict: åŒ…å« content, reasoning_content, finish_reason çš„å­—å…¸
        """
        payload = {
            "model": model,
            "messages": messages,
            "stream": stream,
            **kwargs
        }
        
        # æ·»åŠ æ€è€ƒæ¨¡å¼é…ç½®ï¼ˆå¦‚æœæä¾›ï¼‰
        if thinking:
            payload["thinking"] = thinking
        
        response = self.session.post(
            url=self.BASE_URL,
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            },
            json=payload,
            stream=stream,
            timeout=(10, 300)
        )
        response.raise_for_status()
        
        if not stream:
            return response.json()
        
        for line in response.iter_lines():
            if not line:
                continue
            
            line = line.decode('utf-8')
            if not line.startswith('data: '):
                continue
            
            data = line[6:]
            if data == '[DONE]':
                yield {'finish_reason': 'stop'}
                break
            
            try:
                chunk = json.loads(data)
                choice = chunk.get('choices', [{}])[0]
                delta = choice.get('delta', {})
                
                yield {
                    'content': delta.get('content', ''),
                    'reasoning_content': delta.get('reasoning_content', ''),
                    'finish_reason': choice.get('finish_reason')
                }
            except json.JSONDecodeError:
                continue
    
    def chat(
        self,
        model: str,
        messages: list,
        print_output: bool = True,
        show_reasoning: bool = True
    ) -> tuple[str, Optional[str]]:
        """
        ç®€åŒ–ç‰ˆæµå¼å¯¹è¯ï¼Œè¿”å›å®Œæ•´å†…å®¹å’Œæ€ç»´é“¾
        
        Args:
            model: æ¨¡å‹ä»£ç 
            messages: æ¶ˆæ¯åˆ—è¡¨
            print_output: æ˜¯å¦å®æ—¶æ‰“å°è¾“å‡º
            show_reasoning: æ˜¯å¦æ‰“å°æ€ç»´é“¾
        
        Returns:
            tuple: (æœ€ç»ˆå†…å®¹, æ€ç»´é“¾å†…å®¹)
        """
        full_content = []
        full_reasoning = []
        
        # åˆ¤æ–­æ˜¯å¦æ”¯æŒæ€è€ƒæ¨¡å¼
        thinking_models = ["glm-4.7-flash", "glm-4.6v-flash", "glm-4.1v-thinking-flash"]
        thinking_config = None
        if model in ["glm-4.7-flash", "glm-4.6v-flash"]:
            thinking_config = {"type": "enabled"}
        
        for chunk in self.chat_completion(model, messages, thinking=thinking_config):
            if chunk.get('finish_reason'):
                break
            
            content = chunk.get('content', '')
            reasoning = chunk.get('reasoning_content', '')
            
            if reasoning and show_reasoning:
                full_reasoning.append(reasoning)
                if print_output:
                    print(f"\r[æ€è€ƒ] {''.join(full_reasoning)}", end='', flush=True)
            
            if content:
                full_content.append(content)
                if print_output:
                    print(content, end='', flush=True)
        
        return ''.join(full_content), ''.join(full_reasoning) if full_reasoning else None


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    client = ZhipuStreamingClient("YOUR_API_KEY")
    
    # æ™®é€šæµå¼å¯¹è¯
    content, _ = client.chat("glm-4.7-flash", [
        {"role": "user", "content": "ä½ å¥½"}
    ])
    
    # æ€è€ƒæ¨¡å‹ï¼ˆGLM-4.7-Flashï¼‰
    content, reasoning = client.chat("glm-4.7-flash", [
        {"role": "user", "content": "è®¡ç®— 15 * 23"}
    ])
    if reasoning:
        print(f"\n\næ€ç»´è¿‡ç¨‹: {reasoning}")
    
    # å¼ºåˆ¶æ€è€ƒæ¨¡å‹ï¼ˆGLM-4.1V-Thinking-Flashï¼‰
    content, reasoning = client.chat("glm-4.1v-thinking-flash", [
        {"role": "user", "content": "åˆ†æè¿™ä¸ªé€»è¾‘é—®é¢˜"}
    ])
    if reasoning:
        print(f"\n\næ€ç»´è¿‡ç¨‹: {reasoning}")
```
