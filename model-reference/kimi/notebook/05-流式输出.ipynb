{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05-流式输出\n",
    "\n",
    "Stream 模式使用示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"MOONSHOT_API_KEY\", \"your-api-key\"),\n",
    "    base_url=\"https://api.moonshot.cn/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础流式输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"kimi-k2-turbo-preview\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"你好\"}],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 思考模型流式输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"kimi-k2-thinking\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"解释量子计算\"}],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "print(\"思考过程：\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "in_content = False\n",
    "\n",
    "for chunk in response:\n",
    "    delta = chunk.choices[0].delta\n",
    "    \n",
    "    # 获取 reasoning_content\n",
    "    reasoning = getattr(delta, \"reasoning_content\", None)\n",
    "    if reasoning:\n",
    "        print(reasoning, end=\"\", flush=True)\n",
    "    \n",
    "    # content 在 reasoning_content 之后\n",
    "    if delta.content:\n",
    "        if not in_content:\n",
    "            in_content = True\n",
    "            print(\"\\n\\n最终答案：\")\n",
    "            print(\"-\" * 60)\n",
    "        print(delta.content, end=\"\", flush=True)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 带工具调用的流式输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"获取天气\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\"city\": {\"type\": \"string\"}},\n",
    "                \"required\": [\"city\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"kimi-k2-thinking\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"北京天气如何？\"}],\n",
    "    tools=tools,\n",
    "    max_tokens=16000,\n",
    "    temperature=1.0,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    delta = chunk.choices[0].delta\n",
    "    \n",
    "    reasoning = getattr(delta, \"reasoning_content\", None)\n",
    "    if reasoning:\n",
    "        print(f\"[思考] {reasoning}\", end=\"\")\n",
    "    \n",
    "    if delta.content:\n",
    "        print(delta.content, end=\"\")\n",
    "    \n",
    "    if hasattr(delta, \"tool_calls\") and delta.tool_calls:\n",
    "        for tc in delta.tool_calls:\n",
    "            if tc.function:\n",
    "                print(f\"\\n[工具] {tc.function.name}\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整流式处理类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingChat:\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "    \n",
    "    def chat(self, message, model=\"kimi-k2-thinking\", tools=None):\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": message}],\n",
    "            tools=tools,\n",
    "            max_tokens=16000,\n",
    "            temperature=1.0,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        reasoning_parts = []\n",
    "        content_parts = []\n",
    "        \n",
    "        for chunk in response:\n",
    "            delta = chunk.choices[0].delta\n",
    "            \n",
    "            reasoning = getattr(delta, \"reasoning_content\", None)\n",
    "            if reasoning:\n",
    "                reasoning_parts.append(reasoning)\n",
",
    "                print(f\"\\033[90m{reasoning}\\033[0m\", end=\"\", flush=True)\n",
    "            \n",
    "            if delta.content:\n",
    "                content_parts.append(delta.content)\n",
    "                print(delta.content, end=\"\", flush=True)\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        return {\n",
    "            \"reasoning\": \"\".join(reasoning_parts),\n",
    "            \"content\": \"\".join(content_parts)\n",
    "        }\n",
    "\n",
    "# 使用\n",
    "chat = StreamingChat(client)\n",
    "result = chat.chat(\"今天有什么新闻？\")\n",
    "print(f\"\\n思考: {len(result['reasoning'])} 字符\")\n",
    "print(f\"回答: {len(result['content'])} 字符\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
