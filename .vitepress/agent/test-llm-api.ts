/**
 * ğŸ¤– MetaUniverse LLM API æµ‹è¯•
 * 
 * æµ‹è¯•å¤šå‚å•† LLM Provider æ˜¯å¦å¯ç”¨ï¼šOpenAI, Anthropic, Gemini, æ™ºè°±, DeepSeek, Qwen, Kimi
 * 
 * å‚è€ƒ: model-reference/deepseek/notebook/02-æµå¼è¾“å‡º.ipynb
 * 
 * ä½¿ç”¨æ–¹æ³•:
 * ```typescript
 * import { runLLMAPITests } from './test-llm-api'
 * 
 * // è¿è¡Œæ‰€æœ‰æµ‹è¯•
 * await runLLMAPITests()
 * 
 * // æµ‹è¯•å•ä¸ª Provider
 * import { testDeepSeek } from './test-llm-api'
 * await testDeepSeek()
 * 
 * // æµå¼è¾“å‡ºæµ‹è¯•
 * import { testDeepSeekStream } from './test-llm-api'
 * await testDeepSeekStream()
 * ```
 */

import { loadEnvConfig } from './config/env'
import { createLLMManager, getLLMManager, type LLMManager } from './llm/manager'
import { createLLMConfigFromEnv } from './config/env'
import type { LLMMessage } from './llm/types'

// å­˜å‚¨æµ‹è¯•ç»“æœ
export interface TestResult {
  provider: string
  success: boolean
  model?: string
  response?: string
  tokens?: number
  cost?: number
  error?: string
  duration: number
}

// æµå¼è¾“å‡ºç»“æœ
export interface StreamResult {
  provider: string
  content: string
  reasoning?: string
  tokens: number
  cost: number
  duration: number
}

const results: TestResult[] = []

/**
 * åˆå§‹åŒ– LLM Manager
 */
function initLLMManager(): LLMManager {
  try {
    return getLLMManager()
  } catch {
    const config = createLLMConfigFromEnv()
    return createLLMManager(config)
  }
}

/**
 * æµ‹è¯• DeepSeek
 */
export async function testDeepSeek(): Promise<TestResult> {
  const env = loadEnvConfig()
  const startTime = Date.now()
  
  if (!env.DEEPSEEK_API_KEY || env.DEEPSEEK_API_KEY.includes('your')) {
    return {
      provider: 'deepseek',
      success: false,
      error: 'âš ï¸ æœªé…ç½® DEEPSEEK_API_KEY',
      duration: 0
    }
  }

  try {
    const llm = initLLMManager()
    const response = await llm.chat({
      messages: [{ role: 'user', content: 'å›å¤"DeepSeek OK"' }],
      model: env.DEEPSEEK_MODEL || 'deepseek-chat',
      maxTokens: 20
    }, 'deepseek')

    const duration = Date.now() - startTime
    const result: TestResult = {
      provider: 'deepseek',
      success: true,
      model: response.model,
      response: response.content.slice(0, 50),
      tokens: response.usage.totalTokens,
      cost: response.cost,
      duration
    }
    
    console.log(`âœ… DeepSeek OK (${duration}ms)`)
    console.log(`   Model: ${response.model}`)
    console.log(`   Response: ${response.content.slice(0, 50)}...`)
    console.log(`   Tokens: ${response.usage.totalTokens}`)
    console.log(`   Cost: $${response.cost.toFixed(4)}`)
    
    return result
  } catch (error) {
    const duration = Date.now() - startTime
    const errorMsg = error instanceof Error ? error.message : String(error)
    console.error(`âŒ DeepSeek Error: ${errorMsg}`)
    return {
      provider: 'deepseek',
      success: false,
      error: errorMsg,
      duration
    }
  }
}

/**
 * æµ‹è¯• DeepSeek æµå¼è¾“å‡º
 * å‚è€ƒ: model-reference/deepseek/notebook/02-æµå¼è¾“å‡º.ipynb
 */
export async function testDeepSeekStream(
  onChunk?: (chunk: { content: string; isReasoning?: boolean }) => void
): Promise<StreamResult> {
  const env = loadEnvConfig()
  const startTime = Date.now()
  
  if (!env.DEEPSEEK_API_KEY || env.DEEPSEEK_API_KEY.includes('your')) {
    throw new Error('âš ï¸ æœªé…ç½® DEEPSEEK_API_KEY')
  }

  const llm = initLLMManager()
  
  let fullContent = ''
  let reasoningContent = ''
  let isInContent = false
  
  console.log('ğŸ“ æµå¼è¾“å‡ºï¼š')
  
  await llm.chatStream(
    {
      messages: [{ role: 'user', content: 'è®²ä¸€ä¸ªçŸ­æ•…äº‹' }],
      model: env.DEEPSEEK_MODEL || 'deepseek-chat',
      maxTokens: 500
    },
    (chunk) => {
      const content = chunk.content || ''
      fullContent += content
      
      // æ£€æŸ¥æ˜¯å¦æ˜¯æ€è€ƒè¿‡ç¨‹ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…éœ€æ ¹æ®å“åº”ç»“æ„åˆ¤æ–­ï¼‰
      if (!isInContent && content.trim()) {
        isInContent = true
      }
      
      // å®æ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
      process.stdout?.write?.(content) || console.log(content)
      
      // å›è°ƒé€šçŸ¥
      onChunk?.({ content, isReasoning: false })
    },
    'deepseek'
  )
  
  console.log() // æ¢è¡Œ
  
  const duration = Date.now() - startTime
  const llmManager = getLLMManager()
  const stats = llmManager.getUsageStats()
  
  return {
    provider: 'deepseek',
    content: fullContent,
    reasoning: reasoningContent || undefined,
    tokens: fullContent.length / 2, // ä¼°ç®—
    cost: 0, // æµå¼æš‚æ— æ³•ç²¾ç¡®è®¡ç®—
    duration
  }
}

/**
 * æµ‹è¯• DeepSeek Reasoner æµå¼è¾“å‡ºï¼ˆå«æ€è€ƒè¿‡ç¨‹ï¼‰
 * å‚è€ƒ: model-reference/deepseek/notebook/02-æµå¼è¾“å‡º.ipynb
 */
export async function testDeepSeekReasonerStream(
  onReasoning?: (content: string) => void,
  onContent?: (content: string) => void
): Promise<StreamResult> {
  const env = loadEnvConfig()
  const startTime = Date.now()
  
  if (!env.DEEPSEEK_API_KEY || env.DEEPSEEK_API_KEY.includes('your')) {
    throw new Error('âš ï¸ æœªé…ç½® DEEPSEEK_API_KEY')
  }

  const llm = initLLMManager()
  
  let fullContent = ''
  let reasoningContent = ''
  let isInContent = false
  
  console.log('ğŸ§  æ€è€ƒè¿‡ç¨‹ï¼š')
  console.log('-'.repeat(60))
  
  await llm.chatStream(
    {
      messages: [{ role: 'user', content: 'è¯æ˜å‹¾è‚¡å®šç†' }],
      model: 'deepseek-reasoner',
      maxTokens: 4096
    },
    (chunk) => {
      const content = chunk.content || ''
      
      // ç®€åŒ–å¤„ç†ï¼šå‰åŠéƒ¨åˆ†å¯èƒ½æ˜¯æ€è€ƒè¿‡ç¨‹
      // å®é™…å®ç°éœ€è¦æ ¹æ® API è¿”å›çš„ reasoning_content å­—æ®µåˆ¤æ–­
      if (!isInContent) {
        if (content.includes('è¯æ˜') || content.includes('å®šç†')) {
          isInContent = true
          console.log('\n\nğŸ“„ æ­£å¼ç­”æ¡ˆï¼š')
          console.log('-'.repeat(60))
        } else {
          reasoningContent += content
          process.stdout?.write?.(content) || console.log(content)
          onReasoning?.(content)
          return
        }
      }
      
      fullContent += content
      process.stdout?.write?.(content) || console.log(content)
      onContent?.(content)
    },
    'deepseek'
  )
  
  console.log('\n')
  console.log('='.repeat(60))
  
  const duration = Date.now() - startTime
  
  console.log(`æ€è€ƒéƒ¨åˆ†: ${reasoningContent.length} å­—ç¬¦`)
  console.log(`å›ç­”éƒ¨åˆ†: ${fullContent.length} å­—ç¬¦`)
  
  return {
    provider: 'deepseek-reasoner',
    content: fullContent,
    reasoning: reasoningContent || undefined,
    tokens: (reasoningContent.length + fullContent.length) / 2,
    cost: 0,
    duration
  }
}

/**
 * DeepSeek æµå¼èŠå¤©å°è£…ç±»
 * å‚è€ƒ: model-reference/deepseek/notebook/02-æµå¼è¾“å‡º.ipynb
 */
export class DeepSeekStreamingChat {
  private llm: LLMManager
  
  constructor() {
    this.llm = initLLMManager()
  }
  
  /**
   * æµå¼å¯¹è¯
   * @param message ç”¨æˆ·æ¶ˆæ¯
   * @param model æ¨¡å‹åç§°
   * @param showReasoning æ˜¯å¦æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
   * @param onUpdate å†…å®¹æ›´æ–°å›è°ƒ
   */
  async chat(
    message: string,
    model: string = 'deepseek-chat',
    showReasoning: boolean = true,
    onUpdate?: (chunk: { content: string; isReasoning: boolean; done: boolean }) => void
  ): Promise<{ content: string; reasoning?: string }> {
    const fullContent: string[] = []
    const reasoningContent: string[] = []
    let isInContent = false
    
    await this.llm.chatStream(
      {
        messages: [{ role: 'user', content: message }],
        model,
        maxTokens: model === 'deepseek-reasoner' ? 4096 : 2048
      },
      (chunk) => {
        const content = chunk.content || ''
        
        // å¯¹äº reasoner æ¨¡å‹ï¼Œç®€å•åˆ¤æ–­æ€è€ƒ/å†…å®¹è¾¹ç•Œ
        if (model === 'deepseek-reasoner' && !isInContent) {
          // æ£€æµ‹æ˜¯å¦è¿›å…¥æ­£å¼ç­”æ¡ˆï¼ˆç®€åŒ–é€»è¾‘ï¼‰
          const totalLength = reasoningContent.join('').length
          if (totalLength > 100 && (content.includes('ç»¼ä¸Šæ‰€è¿°') || content.includes('å› æ­¤'))) {
            isInContent = true
            if (showReasoning) {
              console.log('\n')
            }
          } else {
            reasoningContent.push(content)
            if (showReasoning) {
              // ç°è‰²æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
              console.log(`\x1b[90m${content}\x1b[0m`)
            }
            onUpdate?.({ content, isReasoning: true, done: false })
            return
          }
        }
        
        fullContent.push(content)
        process.stdout?.write?.(content) || console.log(content)
        onUpdate?.({ content, isReasoning: false, done: !!chunk.finishReason })
      },
      'deepseek'
    )
    
    console.log()
    
    return {
      content: fullContent.join(''),
      reasoning: reasoningContent.join('') || undefined
    }
  }
}

/**
 * æµ‹è¯• OpenAI
 */
export async function testOpenAI(): Promise<TestResult> {
  const env = loadEnvConfig()
  const startTime = Date.now()
  
  if (!env.OPENAI_API_KEY || env.OPENAI_API_KEY.includes('your')) {
    return {
      provider: 'openai',
      success: false,
      error: 'âš ï¸ æœªé…ç½® OPENAI_API_KEY',
      duration: 0
    }
  }

  try {
    const llm = initLLMManager()
    const response = await llm.chat({
      messages: [{ 
        role: 'user', 
        content: 'Hello, this is a test. Reply with "OpenAI OK"' 
      }],
      model: env.OPENAI_MODEL || 'gpt-4o',
      maxTokens: 20
    }, 'openai')

    const duration = Date.now() - startTime
    const result: TestResult = {
      provider: 'openai',
      success: true,
      model: response.model,
      response: response.content.slice(0, 50),
      tokens: response.usage.totalTokens,
      cost: response.cost,
      duration
    }
    
    console.log(`âœ… OpenAI OK (${duration}ms)`)
    console.log(`   Model: ${response.model}`)
    console.log(`   Response: ${response.content.slice(0, 50)}...`)
    console.log(`   Tokens: ${response.usage.totalTokens}`)
    console.log(`   Cost: $${response.cost.toFixed(4)}`)
    
    return result
  } catch (error) {
    const duration = Date.now() - startTime
    const errorMsg = error instanceof Error ? error.message : String(error)
    console.error(`âŒ OpenAI Error: ${errorMsg}`)
    return {
      provider: 'openai',
      success: false,
      error: errorMsg,
      duration
    }
  }
}

/**
 * æµ‹è¯• Anthropic (Claude)
 */
export async function testAnthropic(): Promise<TestResult> {
  const env = loadEnvConfig()
  const startTime = Date.now()
  
  if (!env.ANTHROPIC_API_KEY || env.ANTHROPIC_API_KEY.includes('your')) {
    return {
      provider: 'anthropic',
      success: false,
      error: 'âš ï¸ æœªé…ç½® ANTHROPIC_API_KEY',
      duration: 0
    }
  }

  try {
    const llm = initLLMManager()
    const response = await llm.chat({
      messages: [{ 
        role: 'user', 
        content: 'Reply with "Anthropic OK"' 
      }],
      model: env.ANTHROPIC_MODEL || 'claude-3-5-sonnet-20241022',
      maxTokens: 20
    }, 'anthropic')

    const duration = Date.now() - startTime
    const result: TestResult = {
      provider: 'anthropic',
      success: true,
      model: response.model,
      response: response.content.slice(0, 50),
      tokens: response.usage.totalTokens,
      cost: response.cost,
      duration
    }
    
    console.log(`âœ… Anthropic OK (${duration}ms)`)
    console.log(`   Model: ${response.model}`)
    console.log(`   Response: ${response.content.slice(0, 50)}...`)
    console.log(`   Tokens: ${response.usage.totalTokens}`)
    console.log(`   Cost: $${response.cost.toFixed(4)}`)
    
    return result
  } catch (error) {
    const duration = Date.now() - startTime
    const errorMsg = error instanceof Error ? error.message : String(error)
    console.error(`âŒ Anthropic Error: ${errorMsg}`)
    return {
      provider: 'anthropic',
      success: false,
      error: errorMsg,
      duration
    }
  }
}

/**
 * æµ‹è¯• Google Gemini
 */
export async function testGemini(): Promise<TestResult> {
  const env = loadEnvConfig()
  const startTime = Date.now()
  
  if (!env.GEMINI_API_KEY || env.GEMINI_API_KEY.includes('your')) {
    return {
      provider: 'gemini',
      success: false,
      error: 'âš ï¸ æœªé…ç½® GEMINI_API_KEY',
      duration: 0
    }
  }

  try {
    const llm = initLLMManager()
    const response = await llm.chat({
      messages: [{ 
        role: 'user', 
        content: 'Reply with "Gemini OK"' 
      }],
      model: env.GEMINI_MODEL || 'gemini-1.5-pro',
      maxTokens: 20
    }, 'gemini')

    const duration = Date.now() - startTime
    const result: TestResult = {
      provider: 'gemini',
      success: true,
      model: response.model,
      response: response.content.slice(0, 50),
      tokens: response.usage.totalTokens,
      cost: response.cost,
      duration
    }
    
    console.log(`âœ… Gemini OK (${duration}ms)`)
    console.log(`   Model: ${response.model}`)
    console.log(`   Response: ${response.content.slice(0, 50)}...`)
    console.log(`   Tokens: ${response.usage.totalTokens}`)
    console.log(`   Cost: $${response.cost.toFixed(4)}`)
    
    return result
  } catch (error) {
    const duration = Date.now() - startTime
    const errorMsg = error instanceof Error ? error.message : String(error)
    console.error(`âŒ Gemini Error: ${errorMsg}`)
    return {
      provider: 'gemini',
      success: false,
      error: errorMsg,
      duration
    }
  }
}

/**
 * æµ‹è¯•æ™ºè°±æ¸…è¨€ (Zhipu)
 */
export async function testZhipu(): Promise<TestResult> {
  const env = loadEnvConfig()
  const startTime = Date.now()
  
  if (!env.ZHIPU_API_KEY || env.ZHIPU_API_KEY.includes('your')) {
    return {
      provider: 'zhipu',
      success: false,
      error: 'âš ï¸ æœªé…ç½® ZHIPU_API_KEY',
      duration: 0
    }
  }

  try {
    const llm = initLLMManager()
    const response = await llm.chat({
      messages: [{ 
        role: 'user', 
        content: 'å›å¤"æ™ºè°±OK"' 
      }],
      model: env.ZHIPU_MODEL || 'glm-4',
      maxTokens: 20
    }, 'zhipu')

    const duration = Date.now() - startTime
    const result: TestResult = {
      provider: 'zhipu',
      success: true,
      model: response.model,
      response: response.content.slice(0, 50),
      tokens: response.usage.totalTokens,
      cost: response.cost,
      duration
    }
    
    console.log(`âœ… æ™ºè°± OK (${duration}ms)`)
    console.log(`   Model: ${response.model}`)
    console.log(`   Response: ${response.content.slice(0, 50)}...`)
    console.log(`   Tokens: ${response.usage.totalTokens}`)
    console.log(`   Cost: $${response.cost.toFixed(4)}`)
    
    return result
  } catch (error) {
    const duration = Date.now() - startTime
    const errorMsg = error instanceof Error ? error.message : String(error)
    console.error(`âŒ æ™ºè°± Error: ${errorMsg}`)
    return {
      provider: 'zhipu',
      success: false,
      error: errorMsg,
      duration
    }
  }
}

/**
 * æµ‹è¯•é˜¿é‡Œäº‘ Qwen
 */
export async function testQwen(): Promise<TestResult> {
  const env = loadEnvConfig()
  const startTime = Date.now()
  
  if (!env.QWEN_API_KEY || env.QWEN_API_KEY.includes('your')) {
    return {
      provider: 'qwen',
      success: false,
      error: 'âš ï¸ æœªé…ç½® QWEN_API_KEY',
      duration: 0
    }
  }

  try {
    const llm = initLLMManager()
    const response = await llm.chat({
      messages: [{ 
        role: 'user', 
        content: 'å›å¤"Qwen OK"' 
      }],
      model: env.QWEN_MODEL || 'qwen-plus',
      maxTokens: 20
    }, 'qwen')

    const duration = Date.now() - startTime
    const result: TestResult = {
      provider: 'qwen',
      success: true,
      model: response.model,
      response: response.content.slice(0, 50),
      tokens: response.usage.totalTokens,
      cost: response.cost,
      duration
    }
    
    console.log(`âœ… Qwen OK (${duration}ms)`)
    console.log(`   Model: ${response.model}`)
    console.log(`   Response: ${response.content.slice(0, 50)}...`)
    console.log(`   Tokens: ${response.usage.totalTokens}`)
    console.log(`   Cost: $${response.cost.toFixed(4)}`)
    
    return result
  } catch (error) {
    const duration = Date.now() - startTime
    const errorMsg = error instanceof Error ? error.message : String(error)
    console.error(`âŒ Qwen Error: ${errorMsg}`)
    return {
      provider: 'qwen',
      success: false,
      error: errorMsg,
      duration
    }
  }
}

/**
 * æµ‹è¯• Kimi (Moonshot)
 */
export async function testKimi(): Promise<TestResult> {
  const env = loadEnvConfig()
  const startTime = Date.now()
  
  if (!env.KIMI_API_KEY || env.KIMI_API_KEY.includes('your')) {
    return {
      provider: 'kimi',
      success: false,
      error: 'âš ï¸ æœªé…ç½® KIMI_API_KEY',
      duration: 0
    }
  }

  try {
    const llm = initLLMManager()
    const response = await llm.chat({
      messages: [{ 
        role: 'user', 
        content: 'å›å¤"Kimi OK"' 
      }],
      model: env.KIMI_MODEL || 'kimi-latest',
      maxTokens: 20
    }, 'kimi')

    const duration = Date.now() - startTime
    const result: TestResult = {
      provider: 'kimi',
      success: true,
      model: response.model,
      response: response.content.slice(0, 50),
      tokens: response.usage.totalTokens,
      cost: response.cost,
      duration
    }
    
    console.log(`âœ… Kimi OK (${duration}ms)`)
    console.log(`   Model: ${response.model}`)
    console.log(`   Response: ${response.content.slice(0, 50)}...`)
    console.log(`   Tokens: ${response.usage.totalTokens}`)
    console.log(`   Cost: $${response.cost.toFixed(4)}`)
    
    return result
  } catch (error) {
    const duration = Date.now() - startTime
    const errorMsg = error instanceof Error ? error.message : String(error)
    console.error(`âŒ Kimi Error: ${errorMsg}`)
    return {
      provider: 'kimi',
      success: false,
      error: errorMsg,
      duration
    }
  }
}

/**
 * æµ‹è¯• DeepSeek Reasoner (æ€è€ƒæ¨¡å¼)
 */
export async function testDeepSeekReasoner(): Promise<TestResult> {
  const env = loadEnvConfig()
  const startTime = Date.now()
  
  if (!env.DEEPSEEK_API_KEY || env.DEEPSEEK_API_KEY.includes('your')) {
    return {
      provider: 'deepseek-reasoner',
      success: false,
      error: 'âš ï¸ æœªé…ç½® DEEPSEEK_API_KEY',
      duration: 0
    }
  }

  try {
    const llm = initLLMManager()
    const response = await llm.chat({
      messages: [{ 
        role: 'user', 
        content: 'è§£æ–¹ç¨‹ï¼š3xÂ² - 6x + 2 = 0' 
      }],
      model: 'deepseek-reasoner',
      maxTokens: 4096
    }, 'deepseek')

    const duration = Date.now() - startTime
    
    console.log(`âœ… DeepSeek Reasoner OK (${duration}ms)`)
    console.log(`   Model: ${response.model}`)
    console.log(`   Response: ${response.content.slice(0, 100)}...`)
    console.log(`   Tokens: ${response.usage.totalTokens}`)
    console.log(`   Cost: $${response.cost.toFixed(4)}`)
    
    return {
      provider: 'deepseek-reasoner',
      success: true,
      model: response.model,
      response: response.content.slice(0, 100),
      tokens: response.usage.totalTokens,
      cost: response.cost,
      duration
    }
  } catch (error) {
    const duration = Date.now() - startTime
    const errorMsg = error instanceof Error ? error.message : String(error)
    console.error(`âŒ DeepSeek Reasoner Error: ${errorMsg}`)
    return {
      provider: 'deepseek-reasoner',
      success: false,
      error: errorMsg,
      duration
    }
  }
}

/**
 * è¿è¡Œæ‰€æœ‰ LLM API æµ‹è¯•
 */
export async function runLLMAPITests(): Promise<TestResult[]> {
  console.log('\n' + '='.repeat(50))
  console.log('ğŸš€ æ‰¹é‡æµ‹è¯•æ‰€æœ‰å·²é…ç½®çš„ LLM API')
  console.log('='.repeat(50) + '\n')

  const results: TestResult[] = []

  // æµ‹è¯•å„ä¸ª Provider
  results.push(await testOpenAI())
  console.log('')
  
  results.push(await testAnthropic())
  console.log('')
  
  results.push(await testGemini())
  console.log('')
  
  results.push(await testZhipu())
  console.log('')
  
  results.push(await testDeepSeek())
  console.log('')
  
  results.push(await testQwen())
  console.log('')
  
  results.push(await testKimi())
  console.log('')

  // æµ‹è¯• DeepSeek Reasonerï¼ˆå¦‚æœ DeepSeek é…ç½®æˆåŠŸï¼‰
  const deepseekResult = results.find(r => r.provider === 'deepseek')
  if (deepseekResult?.success) {
    results.push(await testDeepSeekReasoner())
    console.log('')
  }

  // æ‰“å°æ€»ç»“
  console.log('='.repeat(50))
  console.log('ğŸ“Š æµ‹è¯•æ€»ç»“')
  console.log('='.repeat(50))
  
  const successCount = results.filter(r => r.success).length
  const totalCost = results
    .filter(r => r.success && r.cost)
    .reduce((sum, r) => sum + (r.cost || 0), 0)
  
  console.log(`âœ… æˆåŠŸ: ${successCount}/${results.length}`)
  console.log(`ğŸ’° æ€»æˆæœ¬: $${totalCost.toFixed(4)}`)
  console.log('\nè¯¦ç»†ç»“æœ:')
  
  for (const result of results) {
    const icon = result.success ? 'âœ…' : 'âŒ'
    const duration = result.duration > 0 ? `(${result.duration}ms)` : ''
    console.log(`  ${icon} ${result.provider} ${duration}`)
    if (result.error) {
      console.log(`     ${result.error}`)
    }
  }

  console.log('\n' + '='.repeat(50))
  console.log('âœ¨ æµ‹è¯•å®Œæˆï¼')
  console.log('='.repeat(50))

  return results
}

/**
 * è¿è¡Œæµå¼è¾“å‡ºæµ‹è¯•
 * å‚è€ƒ: model-reference/deepseek/notebook/02-æµå¼è¾“å‡º.ipynb
 */
export async function runStreamTests(): Promise<void> {
  console.log('\n' + '='.repeat(50))
  console.log('ğŸŒŠ æµå¼è¾“å‡ºæµ‹è¯•')
  console.log('='.repeat(50) + '\n')
  
  const env = loadEnvConfig()
  
  if (!env.DEEPSEEK_API_KEY || env.DEEPSEEK_API_KEY.includes('your')) {
    console.log('âš ï¸ æœªé…ç½® DEEPSEEK_API_KEYï¼Œè·³è¿‡æµå¼æµ‹è¯•')
    return
  }
  
  // æµ‹è¯•åŸºç¡€æµå¼è¾“å‡º
  console.log('1ï¸âƒ£ åŸºç¡€æµå¼è¾“å‡º (deepseek-chat)')
  console.log('-'.repeat(50))
  try {
    await testDeepSeekStream()
  } catch (error) {
    console.error('æµå¼æµ‹è¯•å¤±è´¥:', error)
  }
  
  console.log('\n')
  
  // æµ‹è¯•æ€è€ƒæ¨¡å¼æµå¼è¾“å‡º
  console.log('2ï¸âƒ£ æ€è€ƒæ¨¡å¼æµå¼è¾“å‡º (deepseek-reasoner)')
  console.log('-'.repeat(50))
  try {
    await testDeepSeekReasonerStream()
  } catch (error) {
    console.error('æ€è€ƒæ¨¡å¼æµå¼æµ‹è¯•å¤±è´¥:', error)
  }
  
  console.log('\n')
  
  // æµ‹è¯•æµå¼èŠå¤©ç±»
  console.log('3ï¸âƒ£ æµå¼èŠå¤©ç±»æµ‹è¯•')
  console.log('-'.repeat(50))
  try {
    const chat = new DeepSeekStreamingChat()
    const result = await chat.chat(
      'è§£é‡Šé‡å­åŠ›å­¦',
      'deepseek-reasoner',
      true
    )
    console.log('\n')
    console.log('='.repeat(50))
    console.log(`æ€»ç»“: æ€è€ƒ ${(result.reasoning || '').length} å­—ç¬¦, å›ç­” ${result.content.length} å­—ç¬¦`)
  } catch (error) {
    console.error('æµå¼èŠå¤©ç±»æµ‹è¯•å¤±è´¥:', error)
  }
  
  console.log('\n' + '='.repeat(50))
  console.log('âœ¨ æµå¼æµ‹è¯•å®Œæˆï¼')
  console.log('='.repeat(50))
}

/**
 * æµ‹è¯•æ–‡ç« ç”ŸæˆåŠŸèƒ½
 */
export async function testArticleGeneration(topic?: string): Promise<void> {
  const env = loadEnvConfig()
  const defaultProvider = env.LLM_DEFAULT_PROVIDER || 'deepseek'
  
  console.log(`\nğŸ“ æµ‹è¯•æ–‡ç« ç”Ÿæˆï¼ˆä½¿ç”¨ ${defaultProvider}ï¼‰...`)
  
  const testTopic = topic || 'ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ ï¼ˆç®€è¦ä»‹ç»ï¼‰'
  
  try {
    const llm = initLLMManager()
    
    const response = await llm.chat({
      messages: [
        { 
          role: 'system', 
          content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯åšå®¢ä½œè€…ã€‚ä¸ºç»™å®šçš„ä¸»é¢˜ç”Ÿæˆç®€çŸ­çš„æµ‹è¯•æ–‡ç« ï¼ˆ300å­—ä»¥å†…ï¼‰ã€‚' 
        },
        { 
          role: 'user', 
          content: `è¯·å†™ä¸€ç¯‡å…³äº '${testTopic}' çš„ç®€çŸ­ä»‹ç»æ–‡ç« ï¼š` 
        }
      ],
      maxTokens: 500
    })
    
    console.log('âœ… æ–‡ç« ç”ŸæˆæˆåŠŸï¼')
    console.log(`   Tokens: ${response.usage.totalTokens}`)
    console.log(`   Cost: $${response.cost.toFixed(4)}`)
    console.log('\nğŸ“„ æ–‡ç« å†…å®¹é¢„è§ˆï¼š')
    console.log(`${response.content.slice(0, 300)}...`)
  } catch (error) {
    const errorMsg = error instanceof Error ? error.message : String(error)
    console.error(`âŒ æ–‡ç« ç”Ÿæˆå¤±è´¥: ${errorMsg}`)
  }
}

/**
 * æ£€æŸ¥å·²é…ç½®çš„ Providers
 */
export function checkConfiguredProviders(): void {
  const env = loadEnvConfig()
  
  console.log('\nğŸ“‹ ç¯å¢ƒé…ç½®æ£€æŸ¥')
  console.log('='.repeat(50))
  
  const providers = []
  if (env.OPENAI_API_KEY && !env.OPENAI_API_KEY.includes('your')) providers.push('openai')
  if (env.ANTHROPIC_API_KEY && !env.ANTHROPIC_API_KEY.includes('your')) providers.push('anthropic')
  if (env.GEMINI_API_KEY && !env.GEMINI_API_KEY.includes('your')) providers.push('gemini')
  if (env.ZHIPU_API_KEY && !env.ZHIPU_API_KEY.includes('your')) providers.push('zhipu')
  if (env.DEEPSEEK_API_KEY && !env.DEEPSEEK_API_KEY.includes('your')) providers.push('deepseek')
  if (env.QWEN_API_KEY && !env.QWEN_API_KEY.includes('your')) providers.push('qwen')
  if (env.KIMI_API_KEY && !env.KIMI_API_KEY.includes('your')) providers.push('kimi')
  
  console.log(`âœ… å·²é…ç½®çš„ Providers: [${providers.join(', ')}]`)
  console.log(`ğŸ“Š é»˜è®¤ Provider: ${env.LLM_DEFAULT_PROVIDER}`)
  console.log(`ğŸ’° æ¯æ—¥é¢„ç®—: $${env.LLM_DAILY_BUDGET}`)
  
  // æ˜¾ç¤ºå„ Provider é…ç½®è¯¦æƒ…
  console.log('\nè¯¦ç»†é…ç½®:')
  
  if (env.DEEPSEEK_API_KEY && !env.DEEPSEEK_API_KEY.includes('your')) {
    console.log(`  DeepSeek:`)
    console.log(`    API Key: ${env.DEEPSEEK_API_KEY.slice(0, 10)}...`)
    console.log(`    Base URL: ${env.DEEPSEEK_BASE_URL || 'https://api.deepseek.com'}`)
    console.log(`    Model: ${env.DEEPSEEK_MODEL || 'deepseek-chat'}`)
  }
  
  if (env.OPENAI_API_KEY && !env.OPENAI_API_KEY.includes('your')) {
    console.log(`  OpenAI:`)
    console.log(`    API Key: ${env.OPENAI_API_KEY.slice(0, 10)}...`)
    console.log(`    Model: ${env.OPENAI_MODEL || 'gpt-4o'}`)
  }
  
  console.log('='.repeat(50))
}

// é»˜è®¤å¯¼å‡º
export default {
  runLLMAPITests,
  runStreamTests,
  testDeepSeek,
  testDeepSeekStream,
  testDeepSeekReasonerStream,
  DeepSeekStreamingChat,
  testOpenAI,
  testAnthropic,
  testGemini,
  testZhipu,
  testQwen,
  testKimi,
  testDeepSeekReasoner,
  testArticleGeneration,
  checkConfiguredProviders
}
